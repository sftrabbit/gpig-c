\documentclass[10pt,a4paper]{article}


\usepackage[margin=3cm]{geometry}
\usepackage[UKenglish]{babel}
\usepackage{enumitem}
\usepackage{calc}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage[table]{xcolor}
\usepackage{float}
\usepackage{longtable}
\usepackage{parskip}
\usepackage{soul}
\usepackage[small,compact]{titlesec}
\usepackage[justification=centering]{caption}

\definecolor{reqColor}{RGB}{80,80,120}
\definecolor{titleColor}{RGB}{138,201,242}
\pagestyle{fancy}
\lhead{T Davies, A Fahie, A Fairbairn, A Free, J Mansfield, R Tucker, M 
Walker}
\chead{}
\rhead{GPIG-C}
\cfoot{\vspace{-0.6cm} \thepage}

\setlist{nolistsep} % Reduces lots of white space around lists

\renewcommand{\headrulewidth}{0.4pt} % Add rules below header
\renewcommand*{\thefootnote}{\fnsymbol{footnote}}

\newcommand{\conreq}[1]{\textcolor{reqColor}{\textbf{CR.#1}}}
\newcommand{\fr}[1]{\textcolor{reqColor}{\textbf{FR.#1}}}
\newcommand{\frit}[1]{\textit{FR.#1}}
\newcommand{\nfr}[1]{\textcolor{reqColor}{\textbf{NFR.#1}}}
\newcommand{\nfrit}[1]{\textit{NFR.#1}}

\begin{document}

\begin{center}
{\Large GPIG-C Interim Report}

%Word count: \input{wc}\unskip
\footnote{\textit{Using TeXCount, excluding\ldots}} %%Really?

Friday, 14th February 2014
\end{center}

\vspace{0.3cm}
\rule{\textwidth}{0.4pt}
\input{02-glossary}
\input{01-introduction}

\input{03-requirements}
\section{System Architecture}
\label{sec:architecture}

\subsection{Quality Attributes}
\label{sec:architecture-quality}

Quality attributes are defined in order to ensure the architectural decisions 
made when creating the system architecture not only provide the required 
functionality, but also produce a system meeting the expectations of all 
stakeholders. For example the system may be expected to meet certain 
security standards or availability levels. Explicitly defining these quality 
attributes before concretely designing the HUMS allows for the correct 
tactics and patterns to be adopted in order to maximise the utility of important 
qualities. When examining the HUMS non-functional requirements the 
following system quality attributes were identified:
	\begin{center}
	\textit{Flexibility, Performance, Modifiability, Security, 
Testability, Usability}
	\end{center}
For the prototype implementation, at this stage, usability and security 
have not been heavily considered, however in the real system they 
would be of high importance. Security is a key aspect of any HUMS, 
as data being monitored must be both secret and safe. It must be impossible 
for an unauthorised user to insert or remove data from the system, meaning data would need to be stored according to relevant security standards 
(e.g., encryption and password protection for personal data covered by the 
Data Protection Act). Additionally, data in transit between emitters and the HUMS must be protected, which can be achieved using a symmetric-key 
encryption system, which is well supported by many programming languages. 
Determining who has the rights to access which data would fall under the 
purview of an individual customer, who would manage access rights using the 
configuration front-end of our system.

Flexibility is important to the HUMS as it is to be used across multiple 
domains, meaning it cannot be too specific. Following a plugin 
architecture means the system can remain general and flexible however 
can include domain specific elements, such as analysis engines, 
creating a software product line where the core of the HUMS can be 
shared across all domains.

Modifiability is also an important quality of the HUMS and, though not 
measurable, lack of modifiability can increase the cost and time taken to 
complete the project. The HUMS will initially be built to tackle a single 
domain (software), however will in the future be required to work on embedded 
systems, mechanical systems, electrical systems and even people. In 
order to ensure these changes are as efficient as possible the initial system 
must be modifiable. Tactics that can be used to increase the modifiability 
of the system include using small modules within the system, decreasing the 
coupling between those modules and increasing the cohesion. In order 
to achieve low coupling the HUMS can use interfaces between modules, restricting dependencies, such that the majority of modules can be 
swapped out without large changes to the overall system.

The performance of a system is normally measured by its latency and 
throughput. The HUMS is required to dispatch notifications through the 
notification generator after events have been triggered by the analysis 
controller, with a latency less than 5ms. The system is also required to be
able to support multiple data output clients. Tactics such as reducing
overhead though careful use of data structures and prioritising events can
help reduce latency, at least for the most important events, defined by the
end user. It may also be possible to add concurrency, such that multiple
notification threads are used when demand for notifications and reports are
high, reducing the bottleneck created at the notification generator.

In in order to ensure the HUMS is testable we follow the test driven 
development (TDD) methodology where possible, writing and planning the tests 
before writing the code, so a passing test means code performs the 
expected functionality. In addition to this, we also define how 
each requirement of the system would be tested, producing a 
comprehensive test plan, detailing integration, system, unit and 
acceptance testing.

\section{System Views}
\label{sec:architecture-views}

When designing the HUMS we identified four different views of the
system which must be documented, taking inspiration from the
Hofmeister\cite{hof1999} approach to software architecture:

\begin{description}
  \item[Module View] The required software modules and relationships
    between them at a high level.

  \item[Behavioural View] The flow of data and dynamic actions within
    the system.

  \item[Execution View] How modules are geographically positioned.

  \item[Conceptual View] Class diagrams, showing a concrete
    implementation of the functionality.
\end{description}

\subsection{Module View}
\label{sec:architecture-moduleview}

Having determined the quality attributes of the HUMS and the tactics
which can be employed in order to achieve a high utility of these
qualities, a module view can be formed. The component diagram
shown in figure \ref{fig:ComponentDiagram} depicts the modules of the
HUMS and the interactions between these modules at a high level.

%%TODO this diagram doesn't have a key

\begin{figure}[!ht]
  \centering
  \includegraphics[width=14cm]{images/ComponentDiagram.png}
  \caption{The module diagram of the HUMS. Components shown in green are those provided as part of the GPIG-C system. Components shown in blue have default implementations provided as part of the system, but may be added to or replaced by the consumer.}
  \label{fig:ComponentDiagram}
\end{figure}

\begin{description}
  \item[Core] A collection of modules that must be run on one machine, including the data input layer, analysis controller, data abstraction layer, and notification generator. Additional modules can be integrated on the same machine or connected over a network interface.

  \item[Data Emitter] Sends data to the system core in a standard
    format. May be included within the HUMS or created by the end
    user.

  \item[Data Input Layer] Receives data from a data emitter and sends
    it to the data abstraction layer. Alerts the analysis controller
    that new data was received.

  \item[Data Abstraction Layer] Handles all interaction with the
    database, such that if the database was to be altered, only this
    layer would need to change.

  \item[Analysis Controller] Alert analysis engines when new data
    has arrived, pulling the required data from the data abstraction
    layer. If an analysis engine determines an event has occurred
    within the data then the analysis controller forwards that event
    to the notification generator.

  \item[Analysis Engine] Analyses the data looking for trends. If a
    trend is found an event is returned to the analysis
    controller. Analysis engines may be included in the HUMS or
    created by the end user.

  \item[Notification Generator] Receives events from the analysis
    controller, defines an abstract notification, and sends it to the
    appropriate notification engine.

  \item[Notification Engine] Receives an abstract notification from
    the notification generator and creates a concrete notification,
    for example an email or triggering a change in system
    behaviour. Notification engines may be included in the HUMS or
    created by the end user.

  \item[Reports Engine] Pulls data from the data abstraction layer and
    formats it into a report, for example a PDF or graph. Report
    engines may be included in the HUMS or created by the end user.
\end{description}

\subsection{Behavioural View}

The module view shows how modules of the system are connected, however 
does not fully represent the interactions between components and the dynamic actions of the system. A behavioural view can be used to detail this information, showing the flow of data and events within the HUMS. Figure \ref{fig:CommunicationDiagram} shows a communication diagram of the 
HUMS, detailing how the HUMS receives data from a data emitter and generates a notification. The different objects in the system are shown, along with all the interactions and links between them. The messages sent between different modules are shown and labelled with numbers to represent the chronological order in which they occur.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=14cm]{images/CommunicationDiagram.png}
  \caption{Communication diagram showing a behavioural view of message 
flow
through the system, using UML 2 notation}
  \label{fig:CommunicationDiagram}
\end{figure}

%%Input Diagram WITH KEY

\subsection{Execution View}
With a view to deployment and execution, the HUMS needs to be tailorable. The core system may be running on the same hardware as the data emitter or it may be geographically distant; engines may be on separate machines or may all be together. Figures \ref{fig:DeploymentDistributed} and \ref{fig:DeploymentIsolated} show the main scenarios for deployment of the HUMS, with a view that if the system architecture can perform under these situations, then it can perform under all others.

Figure \ref{fig:DeploymentDistributed} shows abstractly how the consumer system could be geographically distant to the HUMS core, engines and datastore. It also shows how a remote admin centre could be used, allowing configuration files to be created at a high level by business personel, as opposed to developers. This deployment possibility gave the team the idea of creating a system as a service (SaaS), where multiple customers can simultaneously use a single storage, analysis, and notification server. We felt this was an innovate addition to the HUMS concept as it removes some of the complexity for consumers in setting up and managing their systems, and allows the benefits of cloud computing to be utilised.

This deployment of our HUMS as a SaaS is shown in figure \ref{fig:DeploymentService}. For administering the HUMS as a service, a hosted admin centre will be created, allowing users to specify analysis and reporting behaviours using a simple rules engine-like interface. Users can define their own events and how they should be handled, view all of the registered input clients, pull reports, and view live graphs of data.

Figure \ref{fig:DeploymentIsolated} shows how we envision the prototype HUMS being deployed at this stage, with all modules on the same machine. It also shows how the system will be deployed on an embedded system, with configuration files being created manually, without an admin centre, and all modules existing on the same device.
\begin{figure}[!ht]
  \centering
  \includegraphics[width=10cm]{images/DeploymentDistributed.png}
  \caption{Deployment diagram showing HUMS spread across multiple 
systems}
  \label{fig:DeploymentDistributed}
\end{figure}

\begin{figure}[!ht]
  \centering
  \includegraphics[width=12.5cm]{images/DeploymentIsolated.png}
  \caption{Deployment diagram showing HUMS on one system}
  \label{fig:DeploymentIsolated}
\end{figure}

\begin{figure}[!ht]
  \centering
  \includegraphics[width=10cm]{images/DeploymentService.png}
  \caption{Deployment diagram showing HUMS as a multi-user service}
  \label{fig:DeploymentService}
\end{figure}

\subsection{Conceptual View}
The conceptual view is a description of the the entire system in terms of its individual components, for example classes in an object-oriented design.

\subsubsection{Data Input Layer}

The data input layer, shown in figure \ref{fig:dataInputLayer},
contains the following classes:

\begin{description}
  \item[DataInputServer] An object which starts a multithreaded
    server to receive connections, and periodically processes
    received data by sending it to the datastore and notifying the
    analysis controller. It accesses these by having references to them
    passed in upon creation.

  \item[ProtoReceiver] Listens on a socket for inbound connections,
    and then hands them off to a new ProtoMultiReceiver, running in a
    new thread, for processing. It also maintains a thread-safe
    concurrent queue, which all receivers write to, and the
    server reads (and removes) from. Use of multithreading
    enables multiple clients to send data at the same time easily, satisfying \nfrit{8}, and
    the use of a queue ensures that data is inserted into the database
    in the order in which it is received.

  \item[ProtoMultiReceiver] Receives data from a particular data input
    client, writing received data to the queue.
\end{description}

\begin{figure}[ht!]
  \centering
  \includegraphics[width=6.2cm]{images/DataInputLayer/DataInputServer.png}
  \includegraphics[width=8.3cm]{images/DataInputLayer/ProtoReceiver.png}
  \includegraphics[width=9.5cm]{images/DataInputLayer/ProtoMultiReceiver.png}
  \caption{Conceptual diagrams of the data input layer classes, using 
UML 2 notation}
  \label{fig:dataInputLayer}
\end{figure}

\subsubsection{Data Abstraction Layer}

The data abstraction layer, shown in figure
\ref{fig:dataAbstractionPackage}, contains the following classes:

\begin{description}
  \item [SystemDataGateway] An interface used to abstract the
    datastore implementation details from the rest of the
    application. An implementation of this interface will provide ways
    to read and write from the chosen datastore, meaning the rest of the
    system is not concerned with the datastore implementation. When
    interfacing with a datastore the end user will be required to
    implement this interface.

  \item [EmitterSystemState] An object representing the data received
    from a data emitter at a particular time. This object holds
    the ID of the end user's system and the creation timestamp along
    with a map of sensor IDs against the value of the sensor at that
    particular time. A map was used to represent the sensors and
    values so that the end user is free to send data from any number of 
    sensors at a time, reducing the number of writes to the
    database. For serialisation this object includes methods to
    convert to and from JSON.

  \item [DataJSONAttribute] An enumeration that wraps up the keys used
    to look up values in the JSON serialisations of the
    EmitterSystemState and QueryResult classes.

  \item [SensorState] An object representing the state of a particular
    system sensor. A sensor has an ID, a value and two timestamps, one
    specifying the time the sensor value was added to the database and
    the other the time the actual value was sensed or created.

  \item [QueryResult] An object representing the result of a database
    query, containing the ID of the end user's system to which the
    data relates and a list of sensor states pulled from the database.
\end{description}

\begin{figure}[ht!]
  \centering
  \includegraphics[width= 9.5cm]{images/DataAbstractionLayer/systemDataGateway.png}
  \includegraphics[width= 5cm]{images/DataAbstractionLayer/dataJsonAttribute.pdf}
  \includegraphics[width= 7cm]{images/DataAbstractionLayer/queryResult.pdf}
  \includegraphics[width= 7cm]{images/DataAbstractionLayer/sensorState.pdf}
  \includegraphics[width= 9cm]{images/DataAbstractionLayer/emitterSystemState.pdf}
  \caption{Conceptual diagrams of the data abstraction layer classes, 
using UML 2 notation}
  \label{fig:dataAbstractionPackage}
\end{figure}

\subsubsection{Analysis Components}

The analysis components of the HUMS, shown in figure
\ref{fig:dataAnalysisComponent}, contain the following classes:

\begin{description}
  \item [AnalysisController] An object that coordinates all analysis of
    sensor data, interfacing with the data input layer by
    receiving a system ID when analysis is required. Once analysis is
    complete, the controller processes the results and triggers
    notifications where appropriate. Analysis engines are associated
    with the analysis controller by class loading allowing for users to
    add and remove analysis engines at run-time.

  \item [AnalysisEngine] An abstract class that implements the basic
    functionality and defines the methods required of an implemented
    analysis engine. The analyse method interfaces with the database
    abstraction layer to retrieve the appropriate sensor data and then
    performs the desired analysis,returning a result object. When creating 
    their own analysis engines the end user will be required to extend 
    this class, implement the abstract methods and define what 
    constitutes an event.

  \item [MeanAnalysis] An analysis engine implementation which 
    computes a rolling average over values read from a sensor over
    a given window. When called by the analysis controller, sensor 
    data is retrieved for all associated systems and mean analysis 
    is performed. If a mean value falls outside of the defined acceptable 
    bounds then a notification is generated.

% TODO DO we even need this object? Surely this is just an event
  \item [Result] \hl{An object representing the result of the analysis
    carried out by an analysis engine, containing a
    map between keys and values to be saved back to the database 
    and a flag indicating whether a notification needs to be triggered.
    When implementing their own analysis engine the Consumer will 
    be able to specify what data is to be saved back to the database.}

  \item [Event] An object representing an event,
    containing the name of the analysis engine that triggered the
    event, the ID of the system that the event applies
    to, and the result object from the analysis engine that triggered
    the event.
\end{description}

\begin{figure}[ht!]
  \centering
  \includegraphics[width= 9.5cm]{images/Analysis/AnalysisController.png}
  \includegraphics[width= 5cm]{images/Analysis/Result.png}
  \includegraphics[width= 12cm]{images/Analysis/AnalysisEngine.png}
  \includegraphics[width= 6cm]{images/Analysis/Event.png}
  \includegraphics[width= 8cm]{images/Analysis/MeanAnalysis.png}
  \caption{Conceptual diagram of the analysis component classes, using 
UML 2 notation}
  \label{fig:dataAnalysisComponent}
\end{figure}

\subsubsection{Notification Components}

Notification components, shown in figure \ref{fig:notificationComponent}, 
contain the following classes:
\begin{description}
  \item [NotificationGenerator] An object that coordinates the generating and
    sending of notifications. The object interfaces with the analysis
    controller, receiving an event object when analysis engines specify that
    notifications should be sent. Notifications are then generated and sent to
    the registered notification engines. Notification engines are associated with
    the notification generator by class loading allowing for users to add and
    remove notification engines at run-time.

  \item [NotificationEngine] An abstract class implementing the basic
    functionality and defining that a notification engine instance must
    override. Notification Engines feature a cool down system ensuring 
    notifications are not sent more then once during a specified period of 
    time.

  \item [EmailNotification] An extension of the notification engine
    class which notifies via email. When called by the notification
    generator, an email notification is sent to the specified recipient, 
    containing the notification subject and any further information.
\end{description}
 
\begin{figure}[ht!]
  \centering
  \includegraphics[width= 7cm]{images/Notification/NotificationGenerator.png}
  \includegraphics[width= 5cm]{images/Notification/NotificationEngine.png}
  \includegraphics[width= 4.5cm]{images/Notification/EmailNotification.png}
  \caption{Conceptual diagram of the notification component classes, using 
UML 2 notation}
  \label{fig:notificationComponent}
\end{figure}

\section{Interim Prototype Implementation}

\subsection{Technologies}

We chose to develop the core of the HUMS prototype in Java. Java can
be compiled to bytecode that can be executed directly in all
environments that support a Java Virtual Machine, including various
operating systems and the x86, x86-64 and ARM CPU architectures,
making it highly portable between both desktop, server and embedded
systems. % REF: http://www.oracle.com/technetwork/java/javase/config-417990.html

Whilst there may exist deployment platforms that are not supported, at
this stage, we do not see this as a problem since the underlying
architecture of the system is transferable across languages.

All members of our team were familiar with the Java programming
language and thus no time was spent having to get the team up to speed
on a new language. We did not impose a language restriction when
developing the data emitter and various engines as the data being
passed between them and the core are language independent, meaning 
end users are free, even with the prototype, to implement engines in any
language they see fit. For the prototype we used both Java and
JavaScript when creating the engines---languages that team members are
familiar with---allowing us to show off the versatility of the system.

Upon deciding to monitor CPU and heap usage, we began to investigate
technologies that would allow us to do so, rather than reinvent the wheel.
Doing this also had the benefit of providing us with established, well-tested
tools with which we could easily implement data input clients for further
systems. As the test program is a Java program, we had to connect to its JVM to query the heap usage, as the operating system would just report the size of the JVM heap. We settled on using SIGAR for process monitoring, and JMX for monitoring heap usage.

An alternative we considered was simply querying the existing features provided
by the operating system and JVM to access this data, however this would have
not been portable across different operating systems, or to embedded systems
with poor support for launching new processes.

To communicate between the core and the other modules, we serialised objects to send them over a network connection. We used JSON and Protocol Buffers. For connections that require data to be serialised to text, we used JSON since it is human readable and maps well to our internal representation of objects in Java. To read and write JSON we used the Jackson library, since it offers a high-performance streaming API. When there was no such restriction on the format of the transmitted data, we used Google Protocol Buffers since they offer high performance, as is necessary for the potentially high-volume connections between the Core and the other modules, such as the Data Emitter and the Analysis Engines.

We initially considered using HTTP rather than Google Protocol Buffers for
communication between the data input layer and the core, however we determined
that sending multiple HTTP requests a second would have significant overhead,
and so we instead opted for a persistent connection and a more lightweight
protocol.

We chose to use Google App Engine to host the prototype's database
module, displaying how easily it can incorporate existing, popular
technologies as plugins. The prototype could easily be extended to
work with other database technologies simply by implementing an
interface within the data abstraction layer.

We created a prototype admin centre to show how the HUMS could be
managed remotely, viewing data and modifying settings online through
a web browser, whilst running as a service. The admin centre, shown in figure \ref{fig:admin} was
developed using the Bootstrap front-end web framework, including 
HTML, JavaScript and CSS, in order to create a prototype realistically
simulating an interface to the HUMS that is accessible on desktop
computers, laptops and mobile devices alike.

\begin{figure}[htbp!]
  \centering
  \includegraphics[width=15cm]{images/admin.pdf}
  \caption{A screenshot of the admin centre homepage}
  \label{fig:admin}
\end{figure}

We used various Java libraries to support our prototype, saving us 
development time and providing a robust codebase. For testing we used 
the Mockito framework to allow us to verify elements of our codebase in 
isolation, by offering a concise way to mock out dependancies.

We used Git for version control, allowing the entire team to
contribute to the project simultaneously, whilst keeping a reversible
history of all alterations made by each person. We considered using a
centralised version control system, such as Subversion, but decided that the
benefits provided by a decentralised system, such as Git, for working
independently and then combining work would outweigh the potential
disadvantages of there being no managing server. As we used GitHub for the
canonical copy of the work, we incorporated many of the benefits of centralised
version control anyway.

\subsection{Current Functionality}
For the interim prototype we chose to firmly follow the planned architecture, producing a core system linking to example engines, emitters and data stores, focussing on interacting with the provided test application.

The core of the HUMS is fully functional, allowing for data emitters to send data to the data input layer using protocol buffers, chosen in preference to HTTP requests due to the lower latency. The data input layer, as currently implemented, supports multiple simultaneous connections from input clients, although we have yet to investigate how many the prototype could handle. Upon receipt, the data is put into a queue, which is periodically processed by the data input server, by adding each entry to the database and notifying the analysis controller. The analysis controller allows for dynamic loading of analysis engines, allowing them to be altered at runtime and requests data to be passed to the appropriate engine, decoupling the analysis engines from the data abstraction layer. The data abstraction layer provides a simple interface to be implemented by the consumer and includes an example implementation, interfacing with Google App Engine using HTTP requests to a remote servlet. The notification generator also allows notification engines to be dynamically loaded and passes event objects to them.

Currently there are two data emitters for use with the system. Demoing its flexibility, the first extracts and emits data about the JVM, recording CPU and memory usage, and the second extracts and emits live data about earthquake activity from the USGS earthquake hazards program API \cite{usgs}.

The data emitter designed to emit data about the JVM records the CPU and memory usage and sends these through the data input API every second. A period of four seconds is allowed to elapse before the first send, allowing the CPU usage readings to stabilise. The data emitter created to relay live data about earthquake activity fetches data every minute from the USGS earthquake hazards program API and submits it to the data input API.

%%Paragraph about analysis plugin 

When the notification generator receives an event, it selects the appropriate notification engines for that event and uses them to create and send a notification. Currently, a single notification engine has been developed which sends an email to an email address configured by the user. This is demonstrated for the test application, where an email will be dispatched when CPU usage exceeds a predefined limit.

%% Paragraph about reports plugins

A prototype admin centre was created for this stage of the project to demonstrate how the HUMS could be public facing as a SaaS, allowing for any company or individual to, for a fee, use the facilities provided; including storage, analysis, notification and reporting tools. Currently the prototype admin centre mocks functionality and --- apart from the reporting examples --- does not actually integrate with the system. We envisage that in the next phase of the project the admin centre will be functional.

\subsection{Source}
The source for our prototype implementation can be accessed online at \verb+http://git.gpigc.co.uk+ and cloned using Git from \verb+http://git.gpigc.co.uk/gpig-c.git+. The username required is \verb+gpigc+ with the password \verb+moustache+.

\subsection{Evaluation of the Prototype Solution for Sample Applications}
\label{sec:prototype-evaluation}

\subsubsection{Customer Test Application -- No. 1}
The Customer provided an example application to monitor as part of the
evaluation of our prototype. With this in mind, we created a monitoring 
package for our prototype solution that can monitor a native application
or one executing on the JVM. We utilised Java Management 
Extensions (JMX) to sample the memory utilisation of the application and
created a process monitoring class which monitors the total
CPU usage of the application. The mean analysis engine computes a 
rolling average of values, and causes an email to be sent if this value
becomes too high, as shown in figure \ref{fig:alerting}. The raw memory
values are read by the reporting system and displayed live in the admin
centre, as shown in figure~\ref{fig:graphing}. The graph shows a value from
every 10 seconds, and automatically updates with the latest values.

% Mean - email
% Raw values - graph

\begin{figure}[htbp!]
  \centering
  \includegraphics[width=7cm]{images/TestApplicationCPUAlert.png}
  \caption{A screenshot of an email notification sent by the mean analysis engine}
  \label{fig:alerting}
\end{figure}

% TODO Talk about performance or meeting the requirements - we've not implemented quite a few things such as feedback?
\begin{figure}[htbp!]
  \centering
  \includegraphics[width=12cm]{images/TestApplicationMemoryGraph.png}
  \caption{A screenshot of the live graphing of the memory usage of the Customer 
  Test Application}
  \label{fig:graphing}
\end{figure}

\subsubsection{Earthquake}
We also created our own sample application: monitoring earthquakes 
through the USGS Earthquake Hazards Program API. The HUMS
can collect live data from the API, store it and plot the location of any
detected earthquakes on a map of the world in the admin centre, as shown
in figure~\ref{fig:plotearthquakes}. We felt that using a map to display
data was a useful and innovative addition.

\begin{figure}[htbp!]
  \centering
  \includegraphics[width=10cm]{images/plotearthquakes.png}
  \caption{A screenshot of the admin centre for our live earthquake monitoring HUMS prototype}
  \label{fig:plotearthquakes}
\end{figure}

\subsubsection{Instructions for Running and Replicating Results}
Once the source has been obtained, it can be run as follows: \hl{Todo - instructions for running the system as a whole: jars and stuff. Joe? I've got stuff going below but FOR THE LOVE OF GOD update the jar filenames and parameters, plus any OS restrictions before completing the document.}

Customer Test Application -- No. 1:
\begin{enumerate}
  \item Run \verb+java -jar custTestAppLauncher.jar+
  \item Observe collected data on \verb+http://www.gpigc.co.uk/admin/map.html+
\end{enumerate}

Earthquake Monitor:
\begin{enumerate}
  \item Run \verb+java -jar earthquakeLauncher.jar+
  \item Observe collected data on \verb+http://www.gpigc.co.uk/admin/graph.html+
\end{enumerate}

\hl{All running Customer Test Application or Earthquake Monitor instances will send data to the same HUMS instance, and so data from multiple instances of the same example application will be interleaved in the admin centre live view. With this in mind, we recommend only running one instance of each application at a time.} % TODO Verify this

The test application memory graph can be viewed at \verb+http://www.gpigc.co.uk/admin/graph.html+ and the earthquake map can be viewed at \verb+http://www.gpigc.co.uk/admin/map.html+.

\subsection{Testing} 
We created and implemented a test plan that laid out our key ideas
with respect to verifying that our solution fulfilled its
requirements. In this section we give the mapping between each
test and its associated requirement. We
defined our plan, where appropriate, in terms of each module within
the system and what specific, testable attributes that module must
have to fulfil its requirements. We also further deconstructed the
testing into various levels of abstraction, mapping the higher level
requirements to their lower level requisite requirements: acceptance
testing, system testing, integration testing and unit
testing.

% TODO We probably need something about overall system & acceptance
% testing. These are mainly the white/black/grey box tests in the
% first report, e.g. NRF.9, "The system must cope with up to 2000 data
% input re- quests per second per HUMS instance."
% Also talk about what we can't test since not everything is implemented

\hl{For system testing, we completed inspection testing, and deployed our system across various networked machines and verified that the HUMS acted as expected. A more thorough system testing stage will be completed for later, higher fidelity, iterations of the prototype.

For acceptance testing, we sent the Customer various iterations of our prototype, and we expect that the feedback from this report will fold back into our acceptance testing.} % Can someone else help expand on this?

Below, the testing within each module is described:

\begin{description}

  \item[Data Emitter] The purpose of the data emitter is to run
    continuously, monitoring a system and reporting on it, thereby
    fulfilling \frit{1.2}. We completed inspection testing to verify 
    that data was reported, along with running the program for 
    several hours to verify that it continued to function.

  \item[Data Input Layer] For the data input layer, it is essential that large
    amounts of data can be received in parallel. We verified this by
    inspection testing, and also by running multiple clients simultaneously and
    verifying that the data from both was recorded, thus verifying \frit{1.2}.

  \item[Data Abstraction Layer] Given the high throughput and
    importance of storage to the function of the HUMS as a whole, the
    data abstraction layer's key attributes are availability and
    performance. We completed an inspection test and used the 
    GWTSystemDataGatewayTest class to verify \frit{3.2}. At the unit 
    testing level, as  part of TDD, we focussed on the comparably complex,
    well defined module-internal functions, such as the generation and 
    parsing of the JSON serialisation of objects.
    
  \item[Analysis Controller] The analysis controller is responsible for 
   coordinating analysis engines and triggering notifications. Due to 
   the central role played by the analysis controller in the analysis 
   pipeline we identified its key attributes as performance and testability. 
   We utilised Mockito to provide completely isolated unit tests, 
   confirming that notifications are only triggered when appropriate and  
   only the relevant analysis engines are used for a given system. 
   Class loading was used to load analysis engines for the prototype, 
   this was confirmed to be working via inspection and integration testing.
  
  \item[Analysis Engine]
   For the analysis engine, we created a unit test to check each aspect of the 
   engine as per \frit{7.1} and \frit{7.4}. We utilised Mockito, a framework 
   for testing which allows mocking of results from methods of a dependancy 
   class to restrict the possible source of failure of a test to be as narrow as 
   possible. This was especially useful for potentially flakey areas, such as 
   database access or connecting over a network that may not be available at 
   test time. Our unit tests covered valid, extreme and null values being sent 
   to the analysis engine.

  \item[Notification Generator] For the notification generator, we created 
	multiple unit tests to verify functionality as per \frit{8}. Mockito was 
	used to isolate the class, allowing for a true unit test. The initial 
	test ensured that a notification is generated when analysis has indicated 
	it. The second test ensured that notifications were only sent once, to 
	avoid spamming users.	 
  
  \item[Notification Engine] The functionality of the email notification 
	engine is tested using a unit test. The test uses the notification engine 
	to send a message to a certain recipient with a given subject and message, 
	then verifies that the email was successfully sent and received by checking 
	the inbox of the specified recipient and inspecting the message to ensure 
	that it contains the correct information.

  \item[Reports Engine] We tested the Reports Engine by inspection since it 
	is a prototype and does not require significant automated testing at this 
	stage.

 \end{description}

\subsubsection{Traceability Table}

Table~\ref{tab:traceability} shows the existing tests used to verify the implementation of the requirements in the system.

\begin{table}[H]
\centering
\begin{tabular}{| p{1.5cm} | p{5cm}| p{1.8cm}| p{4.8cm}|}
  \hline\rowcolor{titleColor} \textbf{Req. ID} & \textbf{Test Classes} & \textbf{Type} & \textbf{Details}\\
  \hline \fr{1.2}   & N/A & Inspection, Alpha & Verify by observing output in admin centre\\
  \hline \fr{2.1}   & SystemDataTest & Unit & Verify existence of timestamp\\
  \hline \fr{2.2}   & EmitterSystemStateTest & Unit & Verify existence of timestamp\\
  \hline \fr{3.1}   & GWTSystemDataGatewayTest & Unit & Valid values\\
  \hline \fr{3.2}   & SystemDataTest,\newline EmitterSystemStateTest,\newline QueryResultTest & Integration, Unit & Read and write to database, create and parse JSON\\
  \hline \fr{7.1}   & AnalysisControllerTest, \newline NotificationGeneratorTest & Inspection,\newline Unit & Check notification flag is set and queried to control notifications\\
  \hline \fr{7.4}   & AnalysisControllerTest & Unit & Verify generation of events \\
  \hline \fr{11}    & EmailNotificationTest  & Unit,\newline Inspection & Verify sending of email notification \\
  \hline \nfr{6}    & N/A  & Inspection & Verify existence of all test levels \\
  \hline
  \end{tabular}
  \caption{The traceability table showing the mapping from each requirement to its existing tests}
  \label{tab:traceability}
\end{table}

\input{06-team}

% TODO:
% -> highlight changes in risk register and development plan - needs to be "appropriately reviewed and revised when necessary"
% -> evidence that the risks have been considered in the planning of the development

\input{07-riskregister}
\input{08-communication}

\vfill
\bibliography{report-refs}
\bibliographystyle{IEEEtran}
\end{document}
