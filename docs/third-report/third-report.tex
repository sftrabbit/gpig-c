\documentclass[10pt,a4paper]{article}

\usepackage[margin=1in]{geometry}
\usepackage[UKenglish]{babel}
\usepackage{enumitem}
\usepackage{calc}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage[table]{xcolor}
\usepackage{float}
\usepackage{longtable}
\usepackage{tabularx}
\usepackage{parskip}
\usepackage{soul}
\usepackage{ifthen}
\usepackage[compact]{titlesec}
\usepackage[justification=centering]{caption}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{xcolor}

\input{listingsPreamble.tex}

\definecolor{reqColor}{RGB}{80,80,120}

%%Tables
\newcommand{\tableformat}[4]{
\begin{table}[ht!]
\centering
  \rowcolors{2}{gray!10} {white}
\def\arraystretch{1.5}
\begin{tabular}{#1}
  \hline
  \rowcolor[gray]{0.9} #2
  \hline
\end{tabular}
\caption{#3}
\label{#4}
\end{table}}

\newcommand{\xtableformat}[4]{
\begin{table}[ht!]
\centering
  \rowcolors{2}{gray!10} {white}
\begin{tabularx}{\textwidth}{#1}
  \hline
  \rowcolor[gray]{0.9} #2
  \hline
\end{tabularx}
\caption{#3}
\label{#4}
\end{table}}

\pagestyle{fancy}
\lhead{T Davies, A Fahie, A Fairbairn, A Free, J Mansfield, R Tucker, M 
Walker}
\chead{}
\rhead{GPIG-C}
\cfoot{\vspace{-0.6cm} \thepage}

\setlist{nolistsep} % Reduces lots of white space around lists

\renewcommand{\headrulewidth}{0.4pt} % Add rules below header
\renewcommand*{\thefootnote}{\fnsymbol{footnote}}

\newcommand{\conreq}[1]{\textcolor{reqColor}{\textbf{CR.#1}}}
\newcommand{\fr}[1]{\textcolor{reqColor}{\textbf{FR.#1}}}
\newcommand{\ed}[1]{\textcolor{reqColor}{\textbf{ED.#1}}}
\newcommand{\nfr}[1]{\textcolor{reqColor}{\textbf{NFR.#1}}}
\newcommand{\qas}[1]{\textcolor{reqColor}{\textbf{QAS.#1}}}

%%Scenarios
\newenvironment{scenario}[1]{
\newcommand{\source}[1]{\item[Source of Stimulus:] ##1}
\newcommand{\stimulus}[1]{\item[Stimulus:] ##1}
\newcommand{\artifact}[1]{\item[Artifact:] ##1}
\newcommand{\environment}[1]{\item[Environment:] ##1}
\newcommand{\response}[1]{\item[Response:] ##1}
\newcommand{\measure}[1]{\item[Response Measure:] ##1}
\newcommand{\rationale}[1]{\item[Scenario Rationale:] ##1}
\newcommand{\quality}[1]{\item[Quality:] ##1}
		\begin{description} [noitemsep]	
		\item[Scenario ID:] \qas{#1}
		}{\end{description} \vspace*{0.3cm}
		}

%%Requirements
\newenvironment{requirements}{
\newcommand{\requirement}[4]{\item[##1{##2}] ##3
							\ifx&##4&
							%nothing
							\else
								\begin{description}
									##4
								\end{description}							
							\fi
							}
		\begin{description}[noitemsep, leftmargin=1.3cm]	
		}{\end{description} \vspace*{0.3cm}
		}
		
\begin{document}
\begin{center}
{\vspace*{-0.5cm}
\Huge GPIG-C Final Report}
\vspace*{0.2cm}

Word count: \input{wc} (\textit{using TeXCount})
\vspace*{0.1cm}

Wednesday, 21st May 2014
\end{center}
\vspace*{0.4cm}
\hrule
\vspace*{0.4cm}

%-------------------------------------------------------------%
%----------------------INTRODUCTION -------------------%
%-------------------------------------------------------------%
\section{Introduction}
\label{sec:intro}
This report details the design and development of our HUMS, concentrating on progress made since the Interim Report. In this report the system requirements are further refined, building on feedback from previous reports and adding a set of external dependencies. The HUMS architecture is then considered in order to determine how the System will meet its requirements and what design decisions must be made. This includes examining the important quality attributes of the system and related scenarios, as well as the tactics and patterns which can be used to achieve these qualities. Our desired HUMS architecture is then presented using a number of views deemed to be the most relevant. Having determined the HUMS architecture, the associated risks of the project as a whole are considered and risk reduction tactics are discussed.

The development of the prototype HUMS, demonstrating some of the important features in the design, is then described, followed by a set of evaluations used to determine the achieved quality and functionality of the System. The project is then concluded, and the team reflects on their decisions throughout the project.

%-------------------------------------------------------------%
%--------------------------GLOSSARY ---------------------%
%-------------------------------------------------------------%
\section{Glossary}
\label{sec:glossary}
\begin{description}%[leftmargin=!,labelwidth=\widthof{\bfseries Data output clientxx},noitemsep]
	\item[The HUMS/System] The health and usage monitoring system being developed.
	
	\vspace{0.15cm}
	\item[Customer] Thales, the organisation that has commissioned the System.
	\item[Consumer] An organisation, or individual within that organisation, using the HUMS.
	\item[Client] Consumer hardware or software that interfaces with the HUMS.
	
	\vspace{0.15cm}
	\item[Event] A trend in data identified by analysis.
	\item[Notification] A message dispatched by the System when an Event is fired.
	\item[Report] An artefact (consisting of HUMS data) produced by the HUMS at the request of the Consumer.
	
	\vspace{0.15cm}
	\item[Input Interface] An interface through which data is supplied to the HUMS.
	\item[Data Emitter] Software that provides Client data to a HUMS instance through an Input Interface.
	\item[Output Interface] An  interface through which Reports and Notifications are dispatched.
	\item[Data Output Client] A Client that receives data from an Instance through an Output Interface.
    \item[Datastore] A component of the System which stores data received through an Input Interface.
    \item[(HUMS) Core] The component of the System which performs analysis, and which receives data from Data Emitters through Input Interfaces.
	\item[Admin Centre] A web interface to configure and interact with the HUMS Core.

	\vspace{0.15cm}
	\item[Sensor] A source of data to be monitored by the System.
	\item[Sensor ID] A unique identifier denoting a particular Sensor.
	\item[System ID] A unique identifier denoting a group of Sensors.
\end{description}

%-------------------------------------------------------------%
%-------------------REQUIREMENTS----------------------%
%-------------------------------------------------------------%
\section{Requirements Refinement}
\label{sec:requirements}
Following feedback from the previous reports, the requirements of the HUMS have been updated. This includes modifying existing requirements and adding additional requirements and refinements. It also includes removing the constraint requirements, which were deemed unnecessary by the module leader, and instead identifying the external dependencies of the system.

\subsection{Functional Requirements}
\label{sec:functional_requirements}

\emph{FR.4.1} has been modified, and refinements added, to increase the configurability of our solution by specifying exactly how configuration files will be used within our implementation. We chose to use JSON as the configuration format because it is a lightweight and widely-used standard.

\emph{FR.12} became apparent after considering the reloading of the configuration in \emph{FR.4.1.2}. Without the ability to dynamically load new engines, modifying registered analysis, storage and reporting methods would require a restart of the System to take effect.

\begin{requirements}
\requirement{\fr}{1}{Data Emitters shall be able to push correctly structured data to the HUMS.}{
	\requirement{\fr}{1.1}{The HUMS shall provide an API for data input (the Input Interface).}{
		\requirement{\fr}{1.1.1}{The Input Interface shall require a System ID that uniquely identifies Clients.}{}
		\requirement{\fr}{1.1.2}{The Input Interface shall require input data to be timestamped.}{}
		\requirement{\fr}{1.1.3}{The Input Interface shall allow Data Emitters to send Sensor IDs and their values to the HUMS, to be made available to an analysis engine.}{}
	}
	\requirement{\fr}{1.2}{Data Emitters for extracting data from the given test applications shall be provided.}{}
}
\requirement{\fr}{2}{The HUMS shall allocate timestamps to new data.}{
	\requirement{\fr}{2.1}{Data shall be timestamped before it reaches the Input Interface.}{}
	\requirement{\fr}{2.2}{Timestamps shall be stored alongside the input data.}{}
}
\requirement{\fr}{3}{The HUMS shall store correctly structured data.}{
	\requirement{\fr}{3.1}{The HUMS shall use a database abstraction layer, allowing the Consumer to select their choice of Datastore technology.}{}
}
\requirement{\fr}{4}{The HUMS shall store Consumer configuration files using JSON.}{
	\requirement{\fr}{4.1}{The HUMS shall allow authorised individuals to modify configuration files.}{
      \requirement{\fr}{4.1.1}{The Admin Centre shall provide an interface to allow authorised individuals to modify the configuration.}{}
      \requirement{\fr}{4.1.2}{The HUMS shall monitor the configuration for changes and apply them.}{}
    }
	\requirement{\fr}{4.2}{The HUMS shall allow the Consumer to define a storage limit.}{}
	\requirement{\fr}{4.3}{The HUMS shall allow the Consumer to set an expiry time on stored data.}{}
	\requirement{\fr}{4.4}{The HUMS shall allow the Consumer to define that, upon reaching their defined data storage quota, new data is no longer stored.}{}
	\requirement{\fr}{4.5}{The HUMS shall allow the Consumer to define that, upon reaching their defined data storage limit, old data is removed to make room for the new data.}{}
}
\requirement{\fr}{5}{The HUMS shall dispatch a Notification when the Consumer's storage limit is reached.}{}
\requirement{\fr}{6}{The HUMS shall store no more data records than the Consumer-defined storage quota.}{}
\requirement{\fr}{7}{Events shall be triggered in response to data matching analysis rules specified by the Consumer.}{
	\requirement{\fr}{7.1}{The HUMS shall allow the Consumer to specify which analysis rules will produce Events.}{}
	\requirement{\fr}{7.2}{The HUMS shall allow the Consumer to define their own Events.}{}
	\requirement{\fr}{7.3}{The HUMS shall provide an API, allowing analysis engines to fetch stored data.}{}
	\requirement{\fr}{7.4}{Events shall be identified by analysis engines.}{}
	\requirement{\fr}{7.5}{The HUMS shall provide a simple analysis engine in the form of a rules engine.}{}
}
\requirement{\fr}{8}{After dispatching a Notification for an Event of a particular type, no more Notifications for an Event of that type shall be sent during a Consumer-specified cool down period.}{
	\requirement{\fr}{8.1}{The HUMS shall allow the Consumer to specify a cool down period for particular Events.}{}
}
\requirement{\fr}{9}{The HUMS shall identify an Event when a specified analysis rule is matched.}{}
\requirement{\fr}{10}{The HUMS shall interface with Report engines, allowing them to pull Reports.}{}
\requirement{\fr}{11}{The System shall allow for Notifications to be fed back to Client to change the way in which data is sensed.}{}
\requirement{\fr}{12}{The System shall load new engines added by an authorised user at both runtime and compile time.}{}
\end{requirements}

\subsection{Non-Functional Requirements}
\label{sec:nonfunctional_requirements}

When considering the non-functional requirements, \emph{NFR.14} was added as it is not necessarily the case that a Consumer will want to use the Admin Centre to modify the configuration file, in which case it should be possible to manually amend the file.

\begin{requirements}
\requirement{\nfr}{1}{The HUMS shall undergo hardware modifications without loss of previously stored data.}{}
\requirement{\nfr}{2}{Consumers shall be provided with documentation detailing how to use the HUMS.}{}
\requirement{\nfr}{3}{The HUMS shall be accessible to Consumers regardless of their geographic location.}{}
\requirement{\nfr}{4}{The HUMS shall only accept data from a Client of the Input Interface providing valid credentials.}{}
\requirement{\nfr}{5}{The HUMS shall allow data to be stored according to the relevant industry security standards.}{}
\requirement{\nfr}{6}{The HUMS shall be tested to ensure all requirements are met before deployment.}{
	\requirement{\nfr}{6.1}{The HUMS shall be tested using unit testing.}{}
	\requirement{\nfr}{6.2}{The HUMS shall be tested using integration testing.}{}
	\requirement{\nfr}{6.3}{The HUMS shall be tested using system testing.}{}
	\requirement{\nfr}{6.4}{The HUMS shall be tested using inspection.}{}
	\requirement{\nfr}{6.5}{The HUMS shall be tested using acceptance testing.}{}
}
\requirement{\nfr}{7}{The Customer shall complete acceptance testing before the System is deployed.}{}
\requirement{\nfr}{8}{The System shall be able to support at least five Clients of the Output Interface per HUMS instance.}{}
\requirement{\nfr}{9}{The System shall be available for no less than 99.9\% of each month.}{}
\requirement{\nfr}{10}{Data shall be backed up within 24 hours of having been made available to the System.}{}
\requirement{\nfr}{11}{Timestamps applied by the system shall be accurate to within 5~ms of UTC.}{}
\requirement{\nfr}{12}{The System shall dispatch Notifications within 5~ms of an Event being triggered.}{}
\requirement{\nfr}{13}{The System shall support storing data without requiring specific schemata.}{}
\requirement{\nfr}{14}{The configuration file shall be in a human-readable format.}{}
\end{requirements}

\subsection{External Dependencies}
\label{sec:external_dependencies}
External dependencies are necessary events which must take place before the HUMS can be run successfully by Consumers. The previous report did not consider the external dependencies of the Consumer running the HUMS as well as those of the development process, so they are considered here.

 \begin{description}
 \item[\ed{1}] An appropriate platform must be provisioned, including supported hardware, operating system and JVM, when not running as a SaaS.
 \item[\ed{2}] The Consumer must be instructed in the process of configuration of the HUMS.
 \item[\ed{3}] The initial Data Emitters, analysis engines, notifiers and data gateways must be selected.
 \item[\ed{4}] Any necessary network or firewall configuration must be performed to allow Clients to communicate with the Core, if not running on the same physical machine.
 \end{description}


%-------------------------------------------------------------%
%--------------------------RISKS--------------------------%
%-------------------------------------------------------------%
\section{Project Risks}
\label{sec:risks}
It is important to consider the risks of any software project so that appropriate mitigation strategies can be applied, and clear contingency plans can be put in place to prevent potential problems having a large impact. This will help to ensure that all work is completed on time and to a high standard. For this phase of the project risks have been identified and mitigated before beginning development, unlike in previous phases where risks were updated ad-hoc which otherwise potentially allowed them to occur before a mitigation strategy is in place. In this iteration many risks were explicitly known, and mitigated, before commencing the next stage of development.

Unlike in the previous reports, risks have been separated into human risks and technological risks. This distinction was not previously made and resulted in technical risks being sidelined and inadequately considered. Human risks are presented in Table~\ref{tab:human_risks} and technical risks in Table~\ref{tab:tech_risks}. Some risks have been modified since the previous report, while others are completely new and specific to the revised system architecture. The risks have each been assigned a risk level corresponding to the product of their likelihood and impact, as described by \cite{risks}, where likelihood is measured on a scale between 1$-$100.

\xtableformat{p{0.8cm} X X X c c c }
{ 	\hline
  	Risk ID & Risk \newline Description & Impact \newline Description & Contingency & Prob. (\%) & Impact & Score \\
  	\hline
  
    R.1 & Loss of team member(s) or team member(s) underperform. & Internal/external deadline failure. Poor standard of work. & Reallocate work across remaining team members and notify module leader. & 10 & Moderate & \textbf{Low} \\
    R.2 & External pressures (such as other modules) limit the amount of development time. & Internal/external deadline failure. Lack of technical achievement. & Modify the project plan to reduce the prototype functionality. Request extension. & 25 & Moderate &  \textbf{Low} \\
    R.3 & Customer does not think that the HUMS design correctly fulfils their brief. & Wasted time. Failure to meet deadlines. Limited prototype implementation. & Modify the system and communicate with the customer to ensure the HUMS design is as they expect. & 15 & Serious &  \textbf{Low} \\
    R.4 & Work is lost due to human or hardware error. & Wasted time. Failure to meet deadlines. Limited prototype implementation. & Revert to a backup and continue working from there, possibly reducing system functionality. & 30 & Minor &  \textbf{Low} \\
    R.5 & The Consumer fails to update the configuration files of the HUMS. & Consumers find the system difficult to use and decide to use a different, simpler HUMS. & Survey potential Consumers to discover what they would find simplest to use and then implement such a system. & 20 & Medium & \textbf{Low} \\	
}
{The human and business risks associated with this phase of the project.}{tab:human_risks}


\xtableformat{p{0.8cm} X X X c c c}
{ 	\hline
    Risk ID & Risk \newline Description & Impact \newline Description & Contingency & Prob. (\%) & Impact & Score \\
  	\hline
  
    R.6 & Developed Admin Centre does not satisfy the outlined requirements. & Poor user experience. Wasted time. Failure to meet requirements. & Redesign Admin Centre. & 10 & Minor & \textbf{Low} \\
    R.7 & Data intercepted between the HUMS Core and registered engines or Datastore. & Loss of confidence in HUMS. Lost/corrupted data. & Additional penetration testing to identify security flaws. Have an audit trail. & 5 & Serious &  \textbf{Low} \\
    R.8 & HUMS does not facilitate data storage using a variety of Datastores. & Failure to meet key modifiability, interoperability and configurability requirements. Low quality solution. & Test this system on a number of popular Datastore implementations and make any required alterations. & 20 & Serious & \textbf{Medium} \\
    R.9 & Computation by included analysis engines too slow. & Failure to meet performance targets. Loss of data due to backlog. & Re-engineer analysis system and reduce complexity. Possibly reduce the functionality of the engines. & 25 & Moderate & \textbf{Low} \\	
    R.10 & Included notification engines too slow. & Failure to meet performance targets. Loss of events, Consumer not informed of System failure. & Re-engineer notification system, reducing complexity. Possibly change protocols and functionality to meet performance requirements. & 20 & Moderate & \textbf{Low} \\	
}
{The technical risks associated with this phase of the project.}{tab:tech_risks}

\subsection{Risk Mitigation}
The following risk reduction techniques have been applied throughout this phase of the project, including both project planning and development, in order to mitigate the identified risks. In addition to the techniques specified in this section, the tactics and patterns explained later in this report have also been used to mitigate technical risks which affect the quality of the system.

\begin{description}
\item[Scrum:]
The use of the scrum as part of our software engineering methodology, as discussed in the Initial Report, helps to mitigate risks \emph{R.1--3}. The scrum allows team members to be kept up to date on what others are working on, and the state of the project as a whole, making it easier for work to be reallocated across team members. It also helps to ensure decisions concerning the HUMS design are discussed and agreed upon, reducing the risk of straying away from the Customer's brief.

\item[Version Control:]
Git version control is used, allowing the entire team to simultaneously modify the project whilst keeping a reversible project history. This helps to mitigate the majority of the identified risks. Human risks are mitigated as the team has access to the work of other members, making it easy to take over when members are unavailable, and preventing large amounts of work being lost due to human or hardware errors. Technical risks are mitigated as it is easy to identify and undo changes which may have caused a problem in the System, or caused the System to not meet Customer expectations.

\item[Automated Testing:] 
Automated integration, unit and regression testing is used throughout the project, using JUnit and Mockito where necessary to ensure the HUMS implementation is stable and free of major bugs. This reduces risks \emph{R.6--10}, helping to automatically validate system requirements. Static testing is also performed using type checkers and code profilers in order to reduce the time spent debugging. % TODO Code profilers don't run on static code, do they?
\end{description}

%-------------------------------------------------------------%
%-----------------------QUALITIES-------------------------%
%-------------------------------------------------------------%
\section{Quality Attributes}
\label{sec:qualities}
A number of quality attributes must be considered when creating the HUMS architecture in order to ensure that architectural decisions not only provide the desired functionality, but produce a system meeting the quality requirements of all stakeholders.
Defining these attributes explicitly allows tactics and patterns to be adopted at an early stage, allowing the system to achieve the desired utilities of each quality. 
Table~\ref{tab:qualities} considers the HUMS, its requirements and its stakeholders, ranking the qualities associated with the system, and explains why each quality was given a particular ranking. The ranking of qualities was not done in the previous report, however this is important as optimising the utility of one attribute may result in the utility of another being reduced. Achieving the right balance is therefore very important, and the utility of lower ranked qualities may need to be sacrificed in order to obtain the required utility of highly ranked qualities. Tailorability has been identified as important to the Customer, and has been decomposed into a number of sub qualities: Modifiability, Configurability, and Interoperability.

\xtableformat{p{2.4cm} p{1.2cm} X}{
\hline
Quality \qquad Attribute & Rank & Reasoning \\
\hline
Configurability & High & It is important that the HUMS can be used in a range of domains; this has been strongly emphasised by the customer. 
\\
Modifiability & High & The HUMS will initially be built to tackle a single domain (software applications), however, in the future it will be used in domains such as embedded, electrical, and mechanical systems. A system which is not modifiable will be timely and costly to extend. In addition, both users and developers need to be able to add custom engines to the HUMS.
\\
Interoperability & High & Interoperability is very important to the HUMS as its main purpose is to interface and exchange data with external systems. 
\\
Availability & High & The non-functional system requirements identified high availability targets for the system. In addition, the HUMS is designed to be used to monitor the health of other systems so if it was not highly available then Consumers would lose confidence in it. The HUMS must be at least as available as the average Consumer's system in order to monitor it properly. 
\\
Security & High & Security is highly ranked as data being monitored must be kept both confidential and safe. It must be impossible for an unauthorised user to view or modify system data, and data in transit between Data Emitters and the HUMS must be protected.
\\
Performance & Medium & The HUMS is required to receive data from Sensors, and produce Notification events in response to that data with a latency no greater than 5 ms. It must also interface with many Data Emitters and Data Output Clients, meaning it must be capable of handling a large number of events quickly.
\\
Testability & Medium & The TDD development approach, discussed in the initial report and followed throughout the project, makes testability of medium importance to the system. Having a high testability utility will help to reduce system faults and is also likely to increase its availability. 
\\
Usability & Medium & Usability was not deemed of great importance to the system as its sub-qualities, such as helpfulness and learnability, have little scope to be optimised in the HUMS. Usability does need to be considered for all Consumer facing interfaces, both graphics and otherwise.
\\
}{The ranked quality attributes associated with the HUMS.}{tab:qualities}


%-------------------------------------------------------------%
%----------------------SCENARIOS---------------------%
%-------------------------------------------------------------%
\section{Quality Attribute Scenarios}
\label{sec:scenarios}
Having identified the quality attributes which are important to creating a successful System, a set of quality attribute scenarios can be created which show how the System should respond to certain stimuli. This helps to create goals which the final implementation can be tested against to determine its quality and how effective it is at achieving its non-functional requirements. The selection of scenarios presented below are included as they appear particularly interesting and describe how the system should behave both normally and in extreme conditions. Other scenarios where also considered and documented, however could not be included due to space limitations.

\begin{scenario}{1}
\quality{Configurability}
\source{The Consumer}
\stimulus{The Consumer wishes to modify the HUMS behaviour by updating their configuration file.}
\artifact{HUMS Core, configuration file}
\environment{Runtime/compile time}
\response{The Consumer is able to modify their configuration file.}
\measure{The HUMS is not reinstalled or recompiled.}
\rationale{This scenario is important to the Customer as they expect the Consumer to be able to easily change HUMS parameters, making it specific to their context, for example modifying the engines their System uses. Achieving this scenario will help our solution to move across different domains easily.}
\end{scenario}

\begin{scenario}{2}
\quality{Modifiability}
\source{Developer}
\stimulus{The Customer wishes to add an additional analysis, reporting, or notification engine to the HUMS.}
\artifact{HUMS Core}
\environment{Runtime}
\response{The new engine is implemented, tested and successfully added to the HUMS, without any loss in service.}
\measure{No other System elements are affected, and the work is completed within their budget.}
\rationale{This scenario is important to both the HUMS developers and the Consumers. The HUMS should allow both of these groups to add custom engines to the System without affecting other System components.}
\end{scenario}

\begin{scenario}{3}
\quality{Security}
\source{An unknown external Client}
\stimulus{The unknown external Client attempts to access stored HUMS data or HUMS services.}
\artifact{HUMS Core}
\environment{Normal Operation -- Online}
\response{The Client is authenticated, and allowed to access System services, or is not recognised and blocked from accessing System data and services.}
\measure{No HUMS data or services are damaged as a result of the access attempt.}
\rationale{This scenario shows how the HUMS is expected to behave when a Client attempts to access its services, showing the importance of authenticating Clients and how the HUMS should respond to unauthorised Clients.}
\end{scenario}

\begin{scenario}{4}
\quality{Availability}
\source{Datastore module}
\stimulus{The Consumer's registered Datastore is unavailable to the HUMS instance.}
\artifact{HUMS}
\environment{Runtime}
\response{A Notification is sent to the administrator of the HUMS instance, and the HUMS periodically attempts to reconnect to the Datastore.}
\measure{End user defined repair time.}
\rationale{This scenario shows how the HUMS should respond to a Datastore failure, clearly drawing a line between the responsibilities of the HUMS and of the Consumer. If the Consumer's defined Datastore fails the HUMS can only alert the user and attempt to reconnect; this is a critical fault and does not allow the System to be run in a degraded mode.}
\end{scenario}


%-------------------------------------------------------------%
%--------------TACTICS / PATTERNS----------------------%
%-------------------------------------------------------------%
%   Notes: 									 %
%  		- Reference risks constantly, lots and lots	 %
%-------------------------------------------------------------%

\section{Tactics and Patterns}
\label{sec:tactics}
Having identified the key quality attributes for the System and a set of quality attribute scenarios, the tactics and patterns needed to ensure the desired utility of these attributes can be examined. In this section we focus on those qualities ranked highly in Table~\ref{tab:qualities}.

\subsection{Tactics}

Below, the tactics which appear most suitable for use with the HUMS are summarised. Each quality attribute has different tactics, which can be used to increase its utility within the HUMS. Configurability tactics are not included as they are covered under modifiability and interoperability, and are complementary. The tactics used are based on the descriptions provided by Software Architecture in Practice~\cite{Bass98}.

\begin{description}
\item[Modifiability] \hfill
	\begin{description}[noitemsep]
	\item[Defer Binding Time] Configuration files, runtime registration and polymorphism will be used to reduce the time taken to make changes, and to allow the Consumer to modify the System at runtime.
	\item[Localise Modifications] Steps are to be taken to maintain semantic coherence within the implementation, with the team aiming to produce loosely coupled modules with high cohesion. Modules are to be generalised where possible, allowing them to preform a larger variety of functions without code changes.
	\item[Prevent Ripple Effect] Communication paths are to be restricted where possible, with the modules which can share data being limited. Information can also be hidden by using private functions, meaning changing implementation details will not affect the public APIs used by the Consumer.
	\end{description}
\item[Interoperability] \hfill
	\begin{description}[noitemsep]
		\item[Locate] Services may need to be discovered at runtime.
		\item[Managing Interfaces] Complex interactions between the HUMS Core and external engines may need to be orchestrated in order to fulfil the required functionality.
	\end{description}
\item[Availability] \hfill
	\begin{description}[noitemsep]
	\item[Fault Prevention] Bundling related actions into transactions should stop the System entering an inconsistent state. If a single action fails within a transaction then all operations will roll back to the previous valid state. Monitoring and shutting down failed processes (e.g. failure of an analysis engine) allows for damage due to erroneous operation to be minimised.
	\item[Fault Detection] Exceptions can be used throughout the software to detect and handle errors. Ping/echo could also be used to ensure external components registered with the HUMS Core are available.
	\item[Fault Recovery] Having active or passive redundant systems would allow for automatic switch over in the event of a System failure, thus minimising interruption to Clients.
	\end{description}
\item[Security] \hfill
	\begin{description}[noitemsep]
	\item[Resisting Attacks] Using prepared statements would mitigate the threat of SQL injection attacks. All connections to the Datastore will require authentication and communications should be encrypted to avoid man-in-the-middle attacks. Access to functionality should be restricted such that no individual has access to services they do not need. 
	\item[Detecting Attacks] Trap files could be used, which should only ever be accessed from specific addresses.
	\item[Recovering From Attacks] Redundant copies of data should be stored, as well as frequent data snapshots, allowing for the HUMS to roll back to a valid state after an attack.
	\end{description}
\end{description}

\subsection{Design Patterns}
Having determined the tactics applicable to the HUMS, which are likely to increase the utility of key system quality attributes, a number of software design patterns can be selected which realise these tactics within its architecture.

\begin{description}
\item[Observer] The observer pattern is a behavioural pattern that defines a one-to-many relationship between a single subject and many observers. Observers are kept in a list within the subject and sent updates when the state of the subject changes. This will be used for the implementation of our analysis, storage, and notification engines, with controller objects keeping a list of engine observers. When the controller receives an update it can inform the relevant engines through a method call.
\item[Pipes and Filters] The adoption of the pipes and filters pattern allows for large processing tasks to be broken down into sequences of smaller independent filters, and the protocol of the data passed between those filters to be explicitly defined. Pipes and filters also increase the tailorability of the HUMS, as alternative architectures can be composed by connecting filters to different pipes.
\item[Table Data Gateway] Utilising table data gateway, defined by \cite{fowler2002patterns} as a single object which handles all interaction with accessing a Datastore (e.g. SQL statements), will reduce coupling and allow a single HUMS to interact with a number of different Datastores. It does by ensuring the Core is not coupled to a particular Datastore implementation, instead each Datastore implements their own table data gateway which can be interchanged both at runtime and compile time. The single point of access and encapsulation means that the System as a whole is unaffected by changes to the Datastore or data access logic and therefore facilitates easier maintenance.
\item[Row Data Gateway] A row data gateway is an object which encapsulates a single record in a Datastore~\cite{fowler2002patterns}. This could be utilised by our HUMS as a method of ensuring data pulled from various Datastores can be used throughout the Core as if it was from a single Datastore. This will reduce the effect of impedance mismatch whilst increasing the tailorability of our solution and decreasing coupling.
\end{description}

\section{System Views} 
Building on our work in the previous reports we decided to present four further, more detailed, views of our HUMS. This time we choose to take inspiration from the SEI~\cite{Bass98} approach to software architecture, including a more detailed \emph{Module View}, a \emph{Component and Connector View}, a dynamic \emph{Behavioural View} and finally a more extensive \emph{Allocation View}.

\subsection{Module View}
\label{sec:views}
A \emph{Module View} of the HUMS, shown in Figure~\ref{fig:modules}, describes our architecture from a structural perspective in terms of a collection of sub-components, each encapsulated so as to minimise the propagation of changes through the system. % Structural perspective

This view is targeted at all System stakeholders, showing enough detail for developers to understand the structure and interactions between components, but not so much detail that non-technical stakeholders would be confused.

In the previous report many of the details in the module view were omitted. In this iteration details of the included and customisable components, such as the engines, are included showing exactly which modules have been implemented so far, as well as their associated interfaces which would be used by the Consumer. 

Figure~\ref{fig:modules} shows a clear separation between those modules included with the HUMS (in blue) and those which may be implemented by the Consumer (in green). The variety and extensiveness of our existing engine implementations already show that our System is highly tailorable, with plenty scope for adding additional functionality.

The design patterns described above can be seen within this view, including the table data gateway pattern which is used for abstracting away the implementation details of particular Datastores, and the observer pattern implemented through the controllers and engines.

The components of the Consumer facing API, made up of generic engines and emitters, are described below:
\begin{description} [noitemsep]	
	\item[Analysis Engines] All Consumer analysis engines must extend the \texttt{AnalysisEngine} abstract class, and therefore implement the \texttt{analyse()} method. This method takes a \texttt{ClientSystem} object, which wraps information about a particular system loaded from the configuration file and returns an \texttt{Event} object generated as a result of the analysis (which may be \texttt{null}). This Event object holds stores the ID of the Client that generated it and key-values describing the event to be used by notification engines. This allows analysis engines free reign as to how they define events, and what information they store about events.
	\item[Notification Engines] Notification engines must extend from the \texttt{NotificationEngine} abstract class, and therefore implement the \texttt{send()} method. The send method takes an \texttt{Event}, representing the result of analysis, and notifies the relevant individual, software or hardware, as defined. Consumers can implement this interface themselves or extend a provided implementation.
	\item[Storage Engines] Storage engines must extend from the \texttt{SystemDataGateway} abstract class, and implement four methods for interacting with a Datastore. There are two methods for writing and two for reading. The \texttt{readMostRecent()} method returns the $n$ most recent records for a given Sensor, and the \texttt{readBetween()} method returns all records for a Sensor within a time period. Both methods for reading return a \texttt{QueryResult} object, which encapsulates the values in the Datastore which fit the query. The methods for writing to a Datastore are both named \texttt{write()}, however one takes a single item of data and writes it to the Datastore and the other is a batch method, used for writing a list of data points more efficiently in the underlying implementation. 
	\item[Data Emitters] Data Emitters read values from Sensors in any format, translate them to a standard format and forward them to the Core for processing. To do this, they must implement the Input Interface. The prototype includes a number of example emitters, such as the Earthquake Emitter and the Test App Emitters, but more may be added by the user. This method of data input allows data from different Clients to pass through the HUMS Core as if it was all the same, vastly increasing our solution's interoperability with different types of Client.
\end{description}

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{images/component.pdf}
  \caption{A \emph{Module View} of the System. Components within the blue areas are included with the HUMS. The green components are those which can be added by the Consumer to extend the HUMS to meet their needs.}
  \label{fig:modules}
\end{figure}

\subsection{Component and Connector View}
In order to visualise how data flows through the System, from its inputs to its outputs, a \emph{Pipes and Filters} diagram of the HUMS has been created. Each filter within the diagram transforms the data received on its input pipe into the data on its output pipes. This diagram, shown in Figure~\ref{fig:pipesAndFilter}, shows the distinct data transformation and processing filters in our solution, as well as the protocol of the pipes connecting them.

Designing this diagram helped us to determine which pipes we must choose protocols for and which can be left for the client to define, improving the tailorability of the system by not unnecessarily restricting the Consumer. This view is again targeted more at technical stakeholders, allowing them to visualise the flow of data, and determine what within the System they can configure.

The input to the HUMS, in this view, is the Sensor data from the Consumer system. This can be passed to the \texttt{:Emit Data} filter in any format, as this is the stage which converts client data to HUMS data. The \texttt{:Emit Data} filter transforms the client data into a format which can be sent using \emph{Protocol Buffers} to the \texttt{:Read Data} filter. The use of \emph{Protocol Buffers} allows for data to be sent locally or over a network, with or without encryption. 

The \texttt{:Read Data} filter deserialises the client data and transforms it to a format which can be used with the Consumer's defined Datastore. The \texttt{:Store Data} filter inserts the data into the Consumers defined Datastore.

The protocol of the pipes used to pass data to the \texttt{:Store Data} and the \texttt{:Create Report} filters are defined by the Consumer, being limited only by what their defined Datastores support. The \texttt{:Create Report} filter transforms data from the Datastore into a \texttt{Data Report}, for example, a graph or PDF.

When sending data between the \texttt{:Store Data} and \texttt{:Analyse Data} filters, \texttt{SensorState} objects are passed, each of which hold the ID of a particular Sensor, the timestamp of when it was created and stored, and its value at the time of creation. The \texttt{:Analyse Data} filter transforms \texttt{SensorState} objects into \texttt{Event} objects, which are used by the \texttt{:Generate Notification} filter to produce Notifications using the engines specified by the Consumer.

An \texttt{Event} object contains the ID of the system which caused the event, and a \texttt{Map} of key-value pairs containing information needed by the registered notification engines. The \texttt{:Generate Notification} filter transforms an \texttt{Event} object into a Notification, for example, an SMS or email. 

\begin{figure}[ht]
  \includegraphics[width=\textwidth]{images/pipesAndFilters.pdf}
  \caption{A \emph{Component and Connector} diagram of the HUMS, showing the protocols used when passing data between filters.}
  \label{fig:pipesAndFilter}
\end{figure}

\subsection{Behavioural View}

The \emph{Behavioural View} shows the actual actions taken by the HUMS when the data flows through the System, displaying the different components and how they interact. Figure~\ref{fig:sequence} shows a sequence diagram that presents this information, detailing the different elements of the HUMS Core and how it behaves during normal operation. Various methods and invocations are listed in the order that they occur, from top to bottom and left to right. The external Clients, such as the sources that data is gathered from and the destinations that Notifications are sent to, are omitted, as this view purely describes the HUMS Core.

\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{images/behaviourView.pdf}
  \caption{A sequence diagram showing the behaviour of the HUMS, using UML 2.0 notation.}
  \label{fig:sequence}
\end{figure}

\subsection{Allocation View}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\textwidth]{images/Isolated.png}
  \caption{\hl{Foo}}
  \label{fig:deployisolated}
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\textwidth]{images/Distributed.png}
  \caption{\hl{Foo}}
  \label{fig:deploydistributed}
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\textwidth]{images/HUMSaaS.png}
  \caption{\hl{Foo}}
  \label{fig:deployhumsaas}
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\textwidth]{images/Scaled.png}
  \caption{\hl{Foo}}
  \label{fig:deployscaled}
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\textwidth]{images/Demo.png}
  \caption{\hl{Foo}}
  \label{fig:deploydemo}
\end{figure}

%-------------------------------------------------------------%
%-------------------DEVELOPMENT-------------------%
%-------------------------------------------------------------%
%   Notes: 					         	 %
%  	- Reference risks constantly, lots and lots	 %
%	- Reference requirements and scenarios 	 %
%-------------------------------------------------------------%

\section{Development}
\label{sec:dev}
In this section, the implementation created in the previous report is summarised and the significant changes made since then detailed. When making these changes our team was structured as described in the previous report. However, we did have to alter our project plan slightly to accommodate other important deadlines. Luckily, we had considered this in risk \emph{R.2} and had appropriate contingency and mitigation strategies in place, meaning we were able to fully utilise the development time available. 

\subsection{Interim Implementation Summary}
\label{sec:interim_summary}
At the end of the Interim Report we had implemented the basic functionality of our Core, with data being sent through the Input Interface and controllers managing engines. However, we did not have dynamic class loading and the Consumer was not able to add classes at runtime. The Consumer was able to create their own engines, but would be required to rebuild the HUMS in order for additions to take effect.

At the interim stage of the project we had created a Data Emitter for the first test application, and an addition Data Emitter monitoring earthquakes. We had implemented a single Datastore using Google App Engine, and had created a basic analysis engine which computed the mean of the latest data for a System and created an \texttt{Event} if that mean was outside define bounds. The parameters used for analysis and to determine system sensors were hard-coded as the configuration file system had not yet been created. An \texttt{EmailNotificationEngine} had also been created which sent emails to a defined address. 

We had created a low fidelity  Admin Centre, which appeared as we had desired, but had no actual functionality, created in just static HTML and JavaScript.

Overall, our interim solution was a low fidelity end-to-end solution, but did not meet all of our requirements and was not impressively tailorable or configurable. 

\subsection{Core Development}
\label{sec:core}
 %giving fully dynamic configurability, that is, it is configurable both at runtime and compile time.
 % and then load the class file at runtime or compile time to extend or modify the default HUMS to meet their needs.
 
The HUMS Core is the main application controlling the behaviour and functionality of the HUMS. It receives Client data through Data Emitters, stores that data and then passes it to appropriate analysis engines which can create \texttt{Event} objects for the Core to pass to relevant notification engines.

\texttt{Controller} objects are used to dynamically load and manage the three different engine types. The addition of dynamic class loading increases the configurability and modifiability of the System by allowing the new engines to be added both at runtime and compile time, making our HUMS fully dynamic.

In addition, the Core now automatically detects when new files are added to the folder they are loaded from, and refreshes the System. This means that from the Consumer's perspective they merely have to place their class into the correct folder to load it, increasing the HUMS's usability.

Several example applications provided with our HUMS are described in this report. These implement standard interfaces provided with the HUMS Core, one for each of the three engine types: storage, analysis and notification. The Customer and Consumer can therefore easily extend and tailor the System by creating their own engines to meet their needs.

The Consumer system and Sensors monitored by a particular HUMS instance, and how they will be monitored, is now defined using a JSON configuration file. The format of this file is displayed in Figure~\ref{fig:configFiles}, showing the definition of a particular Consumer system with a System ID, Sensor IDs, parameters, and which engines the system uses. A System configuration file (\texttt{*.config}) is loaded into the Core by the Consumer before it is run. Changes to configuration files can be made manually through a text editor or in a more user-friendly manner through the Admin Centre.

\begin{figure}[tbp]
\begin{lstlisting}[language=json,firstnumber=1]
{
    "Systems": [
        {
            "SystemID": "EarthquakeMonitor",
            "Sensors": [
                {
                    "SensorID": "EQ",
                    "Params": {
                        "LOWER_BOUND": "1.0"
                    }
                }
            ],
            "Engines": [
                "EarthquakeAnalysisEngine",
                "TwitterNotificationEngine"
            ],
            "DatastoreGateway": "Neo4JSystemDataGateway",
            "Reporting": "Map"
        },
        etc...
    ]
}
\end{lstlisting}
\caption{An example of a JSON configuration file used by the HUMS.}
\label{fig:configFiles}
\end{figure}

Any changes made to the System configuration file whilst the HUMS is running are automatically detected by the Core and cause the file to be reloaded allowing the alterations to take immediate effect. This reduces risk \emph{R.5} by ensuring the Consumer cannot forget to reload the file, increasing the usability of our HUMS by improving its learnability.

The Core has also been refactored to load Consumer system parameters to be used by engines from System configuration files. This includes both individual Sensor parameters and System parameters meaning that, for example, analysis can be performed both per Sensor and on the System as a whole, further increasing the tailorability of our solution. The configuration file for a particular instance of the HUMS can be set and modified at runtime or compile time, as it is separate from the codebase, making it very versatile and further improving configurability. 

In addition to the above, we created an optional GUI for our HUMS Core which makes the process of configuring and running our HUMS even simpler. Shown in Figure~\ref{fig:coreGUI}, this GUI allows the Consumer to select the configuration file they wish to use and to start and stop the Core with the click of a button. The GUI also keeps the Consumer informed of what the Core is doing through an integrated console, reporting any errors such as incorrectly formatted configuration files alongside the general status of the Core. The GUI was created using the \emph{Standard Widget Toolkit}~\cite{swt}. This was chosen over other UI libraries such as \emph{Swing} as it uses native components, increasing user efficiency and satisfaction by using components they are already familiar with.

\begin{figure}[ht]
\centering
\includegraphics[width = 0.8\textwidth]{images/coreGUI.png}
\caption{A screenshot of the HUMS Core GUI.}
\label{fig:coreGUI}
\end{figure}

A lightweight Jetty server was also added to the Core demonstrating that the Admin Centre can be served from the same system that the Core runs on. This enables further integration between the configuration files and the web interface which now allows these files to be easily administered by both technical and non-technical users, and avoids a custom server having to be manually set up by the Consumer as the process is automated.

\subsection{Project Management}
\hl{TODO Joe}

\subsection{Sensing}
\label{sec:monitor}
In this section we describe the applications our HUMS currently monitors. We created six sample Data Emitters for this stage of the project in order to demonstrate the tailorability of our HUMS. These show how the HUMS can integrate with existing applications and APIs, as well as how it can be used with specially made applications. A Consumer would be able to quickly create a Data Emitter for their own application by using the interfaces provided with our HUMS Core.

\subsubsection{Test Application Data Emitters} 
\label{subsec:tapp}
Part of our task was monitoring the provided test applications, treating them as black-box systems we knew nothing about. For the first we simply monitored the memory and CPU usage, but for the second we also added swap state, number of threads, last processor it was run on, and working directory. For this we created a process monitoring Data Emitter which is capable of tracking information from a process. This utilises the \emph{System Information Gatherer} (SIGAR) API. SIGAR is a cross-platform API which allows system information and process information to be read, without worrying about the implementation that the platform provides for reading that information. The information gathered by these engines is forwarded to the Core for analysis.

\subsubsection{Earthquake and Traffic Data Emitters}
In order to ensure our HUMS is capable of receiving, analysing, and reporting data through various external APIs, we updated our earthquake Data Emitter and added an additional traffic Data Emitter, monitoring real time UK traffic accidents and incidents. The earthquake data (from the U.S. Geological Survey~\cite{us_geo}) is formatted as JSON and the traffic data (from the UK Highways Agency~\cite{ha_traffic}) as XML, which are both standard and commonly used data formats. These two emitters demonstrate the extensiblility of the HUMS as similar additional Data Emitters can easily be created to read data from other sources, which would likely also use XML or JSON formats.

\subsubsection{Server Responsiveness}
A common use of existing HUMSs is in monitoring the responsiveness of websites and servers, alerting users and staff if the system becomes unavailable. In order to ensure our HUMS was capable of such a task, we implemented a response time Data Emitter which determines the response time of requests to a given website\footnote{thalesgroup.com} and sends this data to the Core for analysis.

\subsubsection{Emitter Launcher}
For demonstration purposes, and to make our prototype easier to test and use, we created an \emph{Emitter Launcher} which simplifies the process of running our desktop Data Emitters by allowing the individually packaged Emitters to be simultaneously run from a single application, using a GUI. A screenshot of this GUI is shown in Figure~\ref{fig:launcherGUI}, which has again been created using the \emph{Standard Widget Toolkit} \cite{swt}, meaning native components can be used.
 
\begin{figure}[ht]
\centering
\includegraphics*[width=0.8\textwidth]{images/launcherGUI.png}
\caption{A screenshot of the Emitter Launcher GUI.}
\label{fig:launcherGUI}
\end{figure}

\subsubsection{Android Mobile Application}
To explore the possibilities surrounding monitoring remote systems over network communications, we created an Android application with three test systems allowing us to demonstrate that our HUMS is capable of this. Screenshots of the Android application are shown in Figure~\ref{fig:android}.

The first system simulates a car monitoring application, allowing users to select values for a number of Sensors and then push them to the remotely-based Core. In a real system, the user would not set the values or manually push them---it would be automatic---however, we choose to manually update the data for the purposes of demonstrating the functionality.

The second system is a phone battery monitoring application, which allows the user to define how long they would like to use their mobile device for and based on the given battery life, uses the HUMS to determine which services can stay on in order for this target to be met. The HUMS feeds back into the phone system, toggling services as necessary to balance device functionality and battery usage. This system was designed to show how our HUMS is capable of monitoring a system, and feeding information back to alter the system's behaviour in response to the information it collects.

The final system performs face recognition, collecting face data using the built-in device camera, forwarding that data to the HUMS for analysis. This system was included to show how our HUMS allows for fast, complex, and specific data analysis, in this case using computer vision algorithms, in order to show that risk \emph{R.8} has not been realised. 

\begin{figure}[ht]
\centering
\includegraphics[width=4.3cm]{images/phoneApp.png}
\hspace{0.5cm}
\includegraphics[width=4.3cm]{images/carApp.png}
\hspace{0.5cm}
\includegraphics[width=4.3cm]{images/faceApp.png}
\caption{Screenshots of the Android mobile application, showing each of the three subsystems.}
\label{fig:android}
\end{figure}

\subsection{Storing}
\label{sec:store}
This section describes how our HUMS implementation stores data. Our HUMS provides a generic \texttt{SystemDataGateway} interface which can be implemented for a variety of Datastores. To demonstrate this we created \hl{five} sample implementations interfacing with a range of popular storage solutions, including: \emph{Google App Engine}, \emph{PostgreSQL}, \emph{Neo4j}, \hl{\emph{TempoDB}} and \emph{H2}. This was done to show how our System does not force the Consumer to conform to a particular data schema or data storage technology, in fact, the Datastores registered to our sample systems can be altered in the configuration file without any errors occurring. This method increases the overall tailorability of our solution whilst addressing risks \emph{R.7} and \emph{R.8}. The remote Datastores used in our implementation were chosen as they allow \emph{NFR.9--10} be achieved.

Prepared statements were used where possible in the Datastore implementations, being compiled once and then reused, leading to significant gains in performance. Prepared statements also offer improved security against SQL injection attacks, with all user input being treated as data and thus not altering the characteristics of data queries.

\subsubsection{Google App Engine Storage}
\emph{Google App Engine} is a popular cloud based Platform as a Service (PaaS) solution, which is used to deploy applications and manage data. We choose to use this as part of our implementation to demonstrate how our HUMS can easily interface with a remote NoSQL datastore. We did this by creating and hosting a simple web-based application on \emph{Google App Engine}, and then communicating with it using a \texttt{SystemDataGateway} object which performed the relevant HTTP requests.  

\subsubsection{PostgreSQL Storage}
\emph{PostgreSQL} is a prominent relational database management system, which is queried using SQL and is ACID compliant. We included this storage engine in order to show how our HUMS can interact with a remote SQL datastore. To implement this we created a \emph{Heroku} web application, which included a \emph{PostgreSQL} database, and communicated with it from a \texttt{SystemDataGateway} object, again using HTTP.

\subsubsection{H2 Storage}
For real-time applications that require high performance and low response times, a local \emph{H2} instance is offered. \emph{H2} is a Java library that implements a simple, self-contained and server-less transactional SQL database. \emph{H2} can be run in embedded or server mode; its small storage footprint of $1.5$MB and low memory usage make it ideal for use in resource constrained environments. 

\subsubsection{Neo4j Storage}
\emph{Neo4j} is a graph database, where data is stored in nodes connected by types and directed relationships, and as such is suited for analysis which needs to ask questions about the relationships between different pieces of data. \emph{Neo4j} can be distributed across a number of physical systems, providing the capacity for great reliability when necessary.

\subsubsection{TempoDB Storage}
\emph{TempoDB} is a cloud database service for time series data, at a maximum resolution of 1ms. A time series is a collection of data points consisting of a timestamp and some value, and so is suited to analysis such as change in sensor values over a period of time.

\subsection{Analysing}
\label{sec:analysis}
In this section we describe how our HUMS implementation analyses data. We created five sample analysis engines for this stage of the project, demonstrating the flexibility of our HUMS, performing both complex and simple analysis. These engines show how the HUMS can be used for a variety of analysis applications, and how easy it is for the Consumer to add additional engines using the generic \texttt{AnalysisEngine} abstract class provided.

\subsubsection{Bounded Sensor Value Engine}
The previous mean analysis engine has been extended to form a generic bounded sensor value engine. This engine performs per sensor analysis of a Consumer system to determine if the rolling mean average of a number of values is outside of bounds (defined in the system configuration diagram). A single Event is produced detailing any violations across all sensors, generating a single Notification for the system being analysed rather than an unmanageable amount.

The engine is able to analyse any sensor that records numerical values so is reusable across different contexts and flexible for use in a variety of domains.

\subsubsection{Expression Engine}
We created a generic expression engine, \texttt{ExpressionAnalysisEngine}, which allows the configuration file to describe a wide range of potential whole System analysis processes. The expression engine supports most reasonable closed-form expressions, as well as less common helpful features such as conditional expressions. Figure~\ref{fig:exprExample} gives an example of an expression that can be given in the configuration file to define how the engine evaluates a system.
\begin{figure}[ht]
\centering
\verb+if(max(ENGINE_TEMP_A, ENGINE_TEMP_B) >= 120 or RPM > 5000, -1, 0)+
\caption{An example of an expression that evaluates to $-1$ when a sensed system is operating outside of its defined normal operational parameters.}
\label{fig:exprExample}
\end{figure}

The values of variables which represent the state of the system being monitored are available in the expression, and are evaluated lazily for efficiency as evaluation often entails querying the Datastore to retrieve the most recent value which would unnecessarily impact performance.

The implementation of the \texttt{ExpressionAnalysisEngine} works towards addressing the tailorability quality attribute of the solution, particularly its modifiability and configurability, allowing the HUMS to be easily customised by the Consumer without needing any knowledge outside of the analysis engine interface.

\subsubsection{Face Analysis Engine}
\label{sec:fae}

The Android app collects face data, forwarding it to the Core for analysis, where Notifications are generated, notionally granting or denying access to services. This engine shows how components of our HUMS work together to perform complex analysis, which can be tailored using the Core configuration file.

The face recognition process is two fold: an image is captured and its dimensionality is reduced by the Android app before being sent on to the Core, then the \texttt{FaceAnalysisEngine} takes this face description and analyses it to decide if the face is authorised or not.

The face description in the Android app is computed using the \emph{Local Binary Patterns Histogram} (LBPH) \cite{ahonen2006face} method. Each pixel in the image is reduced to a bit-vector describing whether each of the neighbouring pixels is brighter or darker than it. These bit-vectors are then used to generate a histogram. All \emph{uniform bit patterns} (those which have at least two $1$s in a row and no more than two transitions between runs of $1$s and $0$s) have a separate bin and all other patterns have a single bin. A histogram is made for each of a fixed number of regions in the image to ensure that some degree of locality is kept. Each histogram is then concatenated to create the full description vector---the \emph{spatially enhanced histogram}. This results in a compressed description of the image which attempts to preserve the information content pertaining to face description. This method is robust to changes in illumination and pose~\cite{ahonen2006face}, making it a robust method for describing faces in the presence of extraneous variables. This is the data which is then transmitted to the Core for storage and analysis to determine if the face is authorised.

When the histogram data is received by the \texttt{FaceAnalysisEngine}, it is compared against the example authorised faces provided by the configuration file using the \emph{Chi-Squared} distance measure, authorising the face provided by the sensor if its distance from any of the example authorised faces is less than some threshold, also given by the configuration file, and any example unauthorised face.

\subsubsection{Other Analysis Engines}
The \texttt{TrafficAnalysisEngine} creates an Event when a reading is received---the analysis performed in this case is simply parsing the data about the traffic incident and putting it into a readable format for the registered notification engine. The \texttt{EarthquakeAnalysisEngine} is similar, parsing CSV data about an earthquake and then creating an Event only if the magnitude is above a specified value. These engines show how simple the analysis components can be, going to prove that the \texttt{AnalysisEngine} abstract class is incredibly versatile.

\subsection{Reporting and Notifying}
\label{sec:reporting}
This section describes the reporting and notification capabilities of our HUMS. Events created by analysis are used to create notifications through any notification engine configured for a particular HUMS instance. We created four example notification engines to demonstrate a range of different Notifications that might be generated and dispatched using the HUMS, and to show the flexibility and tailorability of the system. They show how the Customer or Consumer could easily create custom notification engines by implementing the generic \texttt{NotificationEngine} abstract class provided, and can easily choose which engines to use for different systems using the configuration file.

For reporting, two sample engines were created which can be used through the Admin Centre. Once requested by the Consumer, these generate reports with live updating information from the HUMS. These engines show just a small selection of what is possible with our HUMS implementation, with scope for adding further web based, standalone, or integrated reporting engines to interact with the HUMS.

\subsubsection{SMS Notifications}
A notification engine was created than sends SMS text messages to mobile phones. This is useful functionality for a HUMS due to the near ubiquitousness of mobile devices in modern life; an individual is likely to be in possession of their phone and can instantly be notified by the HUMS. This is a quick and efficient method of notification that can be used across phone carriers, hardware, and platforms. To create this engine we utilised the service offered by popular communications company Twilio~\cite{twilio}, who have a large infrastructure for telecommunications and offer an easy to use API.

\subsubsection{Email Notifications}
The \texttt{EmailNotificationEngine} provided has the same functionality as the engine included in the Interim Report, however has been heavily refactored to comply with our new, more modular codebase. It can be configured to send email notifications for Events to any email address, and may be useful for more detailed, less frequent Notifications. This has been implemented using the Apache Commons Email~\cite{ac_email} library for Java to interact with an email account over IMAP and SMTP.

\subsubsection{Twitter Notifications}
The \texttt{TwitterNotificationEngine} provided allows Events to be pushed to the social networking site \emph{Twitter}. This was included to demonstrate how our HUMS implementation can be used to interact with popular platforms to report events, such as earthquakes and traffic information, to the masses. The engine creates a tweet from the output of analysis and pushes that tweet to the registered account. This has been implemented using the Twitter4J~\cite{twitter4j} Java library to access the Twitter API.

\subsubsection{Android Application Feedback Notifications}
The \texttt{PhoneNotificationEngine} included with our HUMS, shows how a Consumer system can use to HUMS to alter its own behaviour. It takes the result of analysis on the data collected from the Android phone system, and sends a notification across the network to the phone system, informing it of which services it must turn off in order to meet its battery life target.

\subsubsection{Map Report}
The map reporting engine created in the Interim Report was improved on, making it into a more generic reporting engine that can be used with any latitude and longitude data. This is demonstrated through its use with both the earthquake and the traffic data. Map reports are generated on demand by the Consumer through the Admin Centre, and can then automatically update with the latest HUMS data to provide live information. The map generated uses the Google Maps JavaScript API~\cite{g_maps} which is customised to fit the needs of the report.

\subsubsection{Graph Report}
Similarly to the map reporting, graph reports have been enhanced since the Interim Report and is now a separate engine that can be used to graph different data. The data from both test applications is graphed in the Admin Centre using this engine, showing its flexibility. The graph reports generated can also automatically update to show live data. This report uses the Chart.js JavaScript charting library~\cite{chart_js} to draw and customise the graph.

\subsection{Admin Centre}
\label{sec:admin}

The prototype Admin Centre previously created has been updated and refined to provide a central locations for the Consumer to control the operation and settings of the HUMS. By offering this through a web interface it provides a more familiar and user friendly way for the configuration to be viewed and edited without having to manually edit configuration files. It also means that the Admin Centre can be accessed remotely from a number of different devices and not just from where the core is being run.

The Admin Centre also provides the reporting facilities of the HUMS, with two example reporting engines currently created. A graph report is available and can be used with data like that from the first and second test applications, and a map report is available which is more applicable to data from the earthquake or traffic monitoring.

The updates to the Admin Centre have developed the previous prototype into a fully functioning system that integrates with the HUMS and presents real data and options to the Consumer, allowing them to interact with and update to have effect on the running HUMS instance.

\begin{figure}[h!b]
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{images/admin-centre_sensors.png}
        \caption{Sensors}
        \label{fig:admin-centre_sensors}
    \end{subfigure}
    ~
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{images/admin-centre_engines.png}
        \caption{Engine}
        \label{fig:admin-centre_engines}
    \end{subfigure}
	\vspace{0.2cm}

    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{images/admin-centre_data.png}
        \caption{Datastore}
        \label{fig:admin-centre_data}
    \end{subfigure}
	~
	\begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{images/admin-centre_report.png}
        \caption{Reporting}
        \label{fig:admin-centre_report}
    \end{subfigure}
    \caption{Screenshots of the Admin Centre.}
	\label{fig:admin-centre}
\end{figure}

%-------------------------------------------------------------%
%-----------------------TESTING-----------------------%
%-------------------------------------------------------------%
\section{Testing}
\label{sec:testing}
% this should perhaps be a subsection of the section above or the section below...

We iterated on the test plan created in the Interim Report to help verify that our solution fulfilled its requirements. The test plan has been improved and expanded to also cover new components. In this section we give the mapping between each test and its associated requirement. Where appropriate, the test plan is defined in terms of each module within the system and what specific, testable attributes that module must have to fulfil its requirements. We also further deconstructed the testing into various levels of abstraction, mapping the higher level requirements to their lower level requisite requirements: acceptance testing, system testing, integration testing and unit testing.

For system testing we completed manual and automated testing, such as inspection testing and deploying our system across various networked machines to verify that the HUMS acted as expected. For acceptance testing, we communicated with the customer about various functionality and features of the system, and we expect that the feedback from this report will fold back into our acceptance testing. Unit testing involved automated tests for each different part of the system, and for integration testing we ensured that the disparate components of the HUMS properly interacted and further verified communication across a network.

The testing of each module is described below:

\begin{description}

  \item[Core Shell] The Core Shell provides an interface for the Core of the HUMS and also manages execution. This includes listening for changes to engines, thus fulfilling \emph{FR.12} and \emph{FR.4.1.2}, and managing the use of configuration files, helping to fulfil \emph{NFR.14}. Unit tests and manual inspection were used to verify proper operation.

  \item[Data Emitter] The purpose of the Data Emitter is to run continuously, monitoring a system and reporting on it, thereby fulfilling \emph{FR.1.2}. We completed inspection testing to verify that data was reported, along with running the program for several hours to verify that it continued to function correctly.

  \item[Data Input Layer] For the Data Input Layer, it is essential that large amounts of data can be received in parallel. We verified this by inspection testing, and also by running multiple clients simultaneously and checking that the data from both was recorded, thus verifying \emph{FR.1.1}.

  \item[Data Abstraction Layer] Given the high throughput and importance of storage to the function of the HUMS as a whole, the Data Abstraction Layer's key attributes are availability and performance. We completed an inspection test and used the \texttt{GWTSystemDataGatewayTest} class to verify \emph{FR.3.1}. At the unit testing level, as part of TDD, we focussed on the comparably complex, well defined module-internal functions, such as the generation and parsing of the JSON serialisation of objects.

  \item[Storage Controller] \hl{HOW DO THESE DIFFER FROM THE DAL ABOVE :S}

  \item[System Data Gateway] \hl{HOW DO THESE DIFFER FROM THE DAL ABOVE :S}

  \item[Analysis Controller] Due to the central role played by the Analysis Controller, we identified its key attributes as performance and testability. We utilised Mockito to provide isolated unit tests, confirming that notifications are only triggered when appropriate, satisfying \emph{FR.7} and \emph{FR.9}. Class loading was used to load Analysis Engines for the prototype, this was confirmed to be working via inspection and integration testing.

  \item[Analysis Engine] For the Analysis Engine, we created a unit test to check each aspect of the engine as per \emph{FR.7.1} and \emph{FR.7.4}. Again, we utilised Mockito to mock out dependencies, restricting the possible sources of failure of a test to be as narrow as possible. This was especially useful for potentially flakey areas, such as database access or connecting over a network that may not be available at test time. Our unit tests covered valid, extreme and null values being sent to the Analysis Engine.

  \item[Notification Generator] For the Notification Generator, we created multiple unit tests to verify functionality as per \emph{FR.8}. Mockito was used to isolate the class, allowing for a true unit test. The initial test ensured that a notification is generated when analysis has indicated it. The second test ensured that notifications were only sent once to avoid spamming users.	 

  \item[Notification Engine] The functionality of the email Notification Engine is tested using a unit test. The test uses the Notification Engine to send a message to a certain recipient with a given subject and message, then verifies that the email was successfully sent and received by checking the inbox of the specified recipient and inspecting the message to ensure that it contains the correct information, satisfying \emph{FR.9}.

  \item[Admin Centre] One of the features of the Admin Centre is to modify configuration files, thereby fulfilling \emph{FR.4.1.1}. The Amin Centre is web based so testing was mainly manual.

  \item[Reports Engine] We tested the Reports Engine by inspection since the reports produced are quite visual and vary depending on the data used in them. This reporting ensured \emph{FR.10} was met.

 \end{description}

\subsubsection{Traceability Table}

Table~\ref{tab:traceability} shows the tests used to verify that the system requirements were met.

\tableformat{p{1.4cm} p{4.7cm} p{1.8cm} p{6cm}}
{ 	\hline
    Req. ID & Test Classes & Type & Details \\
    \hline

    FR.1.1   & N/A & Inspection & Use Datastore consoles and viewers to verify data storage \\
    FR.1.2   & N/A & Inspection & Verify by observing output in Admin Centre \\
    FR.2.1   & SystemDataTest & Unit & Verify existence of timestamp \\
    FR.2.2   & EmitterSystemStateTest & Unit & Verify existence of timestamp \\
    FR.3   & SystemDataTest,\newline EmitterSystemStateTest,\newline QueryResultTest & Integration, Unit & Read and write to database, create and parse JSON \\
	FR.3.1   & GWTSystemDataGatewayTest, \newline H2SystemDataGatewayTest, \newline HerokuSystemDataGatewayTest & Unit & Verify that different Datastores work correctly and interchangeably \\
	FR.4.1.1 & N/A & Inspection & Verify configuration can be changed through the Admin Centre \\
	FR.4.1.2 & ConfigParserTest & Unit, \newline Inspection & Verify changes to the configuration file are detected and acted upon \\
	FR.7     & AnalysisEngineTest, \newline DataEventTest & Unit & Verify events are triggered by analysis where appropriate \\
    FR.7.1   & AnalysisControllerTest, \newline NotificationGeneratorTest & Unit,\newline Inspection & Check notification flag is set and queried to control notifications \\
    FR.7.4   & AnalysisControllerTest & Unit & Verify generation of events \\
	FR.8     & NotificationEngineTest & Unit & Verify the specified notification cool down period is adhered to \\
    FR.9     & EmailNotificationTest, \newline SMSNotificationTest, \newline TwitterNotificationTest & Unit,\newline Inspection & Verify sending of notifications \\
	FR.10    & N/A & Inspection & Verify appropriate graph and map reports can be generated with correct data \\
	FR.12    & N/A & Inspection & Verify that the Core detects new engines and informs that they are loaded \\
    NFR.6    & N/A & Inspection & Verify existence of all test levels \\
	NFR.14   & ConfigParserTest, \newline ParameterTest, \newline ClientSensorTest, \newline ClientSystemTest & Unit, \newline Inspection & Verify that configuration files are understandable and parseable \\
}
{The traceability table showing the mapping from each requirement to its existing tests.}{tab:traceability}


%-------------------------------------------------------------%
%-----------------------PROTOTYPE-----------------------%
%-------------------------------------------------------------%
\section{Prototype Implementation Evaluation}
\label{sec:prototype}
\hl{INTRO: Rosy||Joe||Andy}

\subsection{Evaluation Against Requirements, Qualities, and Scenarios}
\label{sec:req_eval}

% -- Qual attibrs
% High configurability
% High modifiability
% High Interop
% High Avail
% High Security
% Medi Performance
% Medi Testability
%  Low Usability

% -- scenario qual atr
% QAS1 The Customer wishes to reconfigure the system to monitor mechanical systems.
% QAS2 The Customer wishes to add an additional analysis, reporting, or notification engine to the HUMS.
% QAS3 The Client is authenticated, and allowed to access system services, or is not recognised and blocked from accessing system data and services.
% QAS4 A Notification is sent to the administrator of the HUMS instance, and the HUMS periodically attempts to reconnect to the Datastore

\hl{Have we met requirements, why? Is our architecture just not capable of it? Or have we just not implemented because its a prototype, and here's how we'd do it.... Is this addressed by the table? Find the requirement about a manual for setting up the HUMS and link to our manual as the validation}

To consider the meeting of requirements, we lay out in Tables \ref{frevaltable} and \ref{nfrevaltable} the steps which we have taken in order to validate the meeting of the requirements. \emph{FR.5}, \emph{FR.6}, and \emph{FR.8} were not implemented in this prototype, and so the method of validation describes what would be done in a more complete system.

% ID/Method of Validation/Result
\xtableformat{ p{1.3cm}  X X }{ 
	FR.x  &  	Method of Validation & Result \\ \hline
	FR.1  & 	We created an API for data input which uses a system ID to determine which input is sending the data. The information is sent to the core utilising a data emitter and this data emitter can read the sensor ID and pass the information into the analysis engine for analysis. & \\
	FR.2  & Timestamps are held for all data, and stored in the Datastore when the information is saved. 	  & 	\\
	FR.3  & To meet the requirement to store correctly structured data, we have enforced checks on data entering the core. The data is stored in a database. This is H2 for a local database, and is a Postgres instance on Heroku for the cloud solution. 	  & 	 \\ 
	FR.4  &This requirement has been satisfied by the use of configuration files, these can be loaded into the core at runtime. And can easily be changed and reloaded while the core is running. The administration centre can be used to edit these configuration files even while the system is running.	  & 	 \\ 
	FR.5  & This would be verified by submitting a large amount of data to the system and verifying that a notification was dispatched & 	 \\
	FR.6  & This would be validated by submitting a large amount of data to the system and verifying that it behaved correctly.  & 	  \hl{?} \\ 
	FR.7  & We implemented a number of Data Input Clients and analysis engines, and verified that the analysis engines could send notifications when the input matched some configuration-defined pattern  & 	  \hl{?} \\ 
	FR.8  & This would be verified by triggering a large number of events and observing whether a configuration-defined cooldown was obeyed.  & 	 \\ 
	FR.9  & For recognition of an event, the information which is received from a system ID is passed into the core and analysed. If analysis rules are matched, then a notification is generated for the event. 	  & 	\\
	FR.10  & 	  & 	 \\ 
	FR.11  & 	  & 	 \\ 
	FR.12  &This can be achieved by the user placing them in the correct namespace in JAVA. The system will load all the classes associated with the engines when it first loads. 	  & 	 \\ 
}{Evaluation for satisfaction of functional requirements.}{frevaltable}

\xtableformat{ p{1.3cm} X X }{
	NFR.x  & 	Method of Validation & Result \\ \hline
	NFR.1  &The Datastore is not within the HUMS hardware, but is instead user defined. Hence, the system is capable of keeping data persistently even when the hardware the HUMS is operated on is changed. 	  & 	 \\ 
	NFR.2  &There has been a README provided. 	  & 	 \\ 
	NFR.3  & 	The HUMS system can be utilised regardless of geographic location, as the main monitoring site is accessible from the web. Furthermore, the emitters can be run detached from the main system, for example on an embedded system. 	  & 	 \\ 
	NFR.4  &We did not implement a security system in the current implementation, however for the full implementation which would be for release we had a set of requirements in place which we would enforce to provide an acceptable level of security. 	  & 	\\ 
	NFR.5  & For the prototype system the data is being stored in a database. This does not meet this requirement, but is good enough for our prototype. For the final product that would be released, where security was needed, this storage method would have to be enhanced. An example method for achieving this is using file-system encryption. & \\
	NFR.6  &The tests for the system have been created and included at each stage of the development. \hl{write smat stuff} 	  & 	\\
	NFR.7  &The system has not currently been performed. However, in the case of the real system, this would be performed by the customer and the customer would sign off to show the requirements were met. 	  & 	 \\
	NFR.8  &The current implementation is executing with 5 clients. With the emitter launcher we have tested it with 5 clients and the performance is correct. There have been tests with more clients and the functionality remains correct.	  & 	 \\ 
	NFR.9  & The Heroku instance has an uptime which satisfies this requirement.	  & 	 \\ 
	NFR.10  & \hl{data backup - adam?}	  & 	 \\ 
	NFR.11  & 	  & 	 \\ 
	NFR.12  & 	  & 	 \\ 
	NFR.13  & 	To achieve this, a key-value data store format has been used. This relaxes the need for a schema to a simple pair of key to value.  & 	 \\
	NFR.14  & 	The format for the configuration files is JSON, this is a widely-adopted and easy to modify format for update by a human.  & 	 \\
}{Evaluation for satisfaction of non-functional requirements.}{nfrevaltable} % TODO What should we do when this spills over a page? Just make a long table or is there something smarter?

We set out to test the degree to which the HUM fulfilled the required quality attributes. As an example, to test the usability of the system, we performed an informal user experience study in which members of the department were asked to interact with the system and provided feedback. Results of the study influenced our later iterations of our implementation, including adding a monospace font and clear button to our console, and a way to control engines and other plugins from a web interface, rather than directly interacting with the system configuration file.

In our final prototype implementation, the quality attributes laid out in Section~\ref{sec:scenarios} have been largely satisfied. The requirement that the HUMS system be utilisable in multiple domains was emphasised strongly, and so high configurability was a goal from the beginning of the project. One are important to the customer was the ability to monitor mechanical systems. This can be achieved using our HUMS system and a stand-alone Datastore to log the information. This would work in a similar manor to a black box recorder, and the data could be easily retrieved when needed. The notification system could easily be adapted to turn, for example, an LED off/on when an error had occurred.

\hl{Scenarios: Response and measure? What we expected and what we achieved}

A scenario for evaluating the modifiability is the ability to add engines for analysis, reporting or notifications. This has been achieved using extensible APIs for the analysis, reporting and notification engines. Each of the engines contains an abstract base class which can be extended to provide a new engine. For example, to implement a new analysis engine one must simply extend the AnalysisEngine class and implement the required abstract methods.

To evaluate the security, we created a scenario whereby a unknown client would attempt to access data stored in the HUMS or would attempt to insert data into the HUMS. Whilst we have not implemented the security system in the deliverable, we have theorised about how we would go about it and if we had more time in order to implement the full system, we have stated how we would achieve a given level of security.

\subsection{Evaluation on Test Application 1}
\label{sec:test_app1}

We decided to monitor the memory and CPU usage of the test application, and our initial implementation simply queried the operating system for these values, using the SIGAR library. However, this did not work as the JVM manages its own heap internally. We then changed to directly querying the target JVM for its memory usage using the JMX libraries, which was successful.

By using these cross-platform and established libraries, our monitor can be generalised to more than just the test application. This is illustrated by how easily we were able to reuse it as the basis of the second test application.

\subsection{Evaluation on Test Application 2}
\label{sec:test_app2}

Using iotop~\cite{iotop} under Ubuntu 14.04, we were able to verify that the test application did not perform any notable IO operations. Using Wireshark under the same system, no network activity was detected either.
We decided to monitor the process memory usage, CPU usage, state (idle, running, sleeping, stopped, or zombie), working directory, number of threads, and which processor it was last run on. We were able to base the monitor on that developed for the first test application with only minor changes. We decided to monitor the threads because none of the other metrics produced an interesting result, however we found that only one thread was used.

As this is an extension of the previous work for the first test application, this can be easily adapted as a cross-platform solution for monitoring any process running on a given system. 

\subsection{Evaluation of the HUMS SaaS}
\label{sec:hums_saas}

In the Interim Report we proposed an innovative deployment option for our HUMS as a System as a Service (SaaS) solution. This would allow multiple Consumers to use the same central HUMS, saving them from the complexity of setting up and managing the system themselves, whilst still allowing flexibility for them to configure it to their needs and interface with local Datastores and engines.

This is possible due to the ability of different components to run from different location, for example a data emitter can run on a different system to the Core of the HUMS. With our HUMS implementation the SaaS solution could be achieved by running multiple Cores on the same server, with one Core for each Consumer. This is not ideal, as a new Core instance for a new Consumer would have to manually be started, but does successfully enable this type of deployment.

Ideally, only a single instance of the Core would be needed and would be able to handle multiple Consumers' configurations. This is a future possibility for the system as only small parts of the Core would need to be changed and all other components, including all engines, would still work as before.

Along with SaaS, a Platform as a Service (PaaS) deployment is also possible with our implementation. The HUMS would be provided to the Consumer so they could configure it as needed and then deploy it using servers and storage that we would provide. They would also be able to bundle in their own custom engines and any additional software to be deployed and run alongside the HUMS on our platform, which would further reduce the the complexity and cost to the Consumer of purchasing and managing their own hardware. This could not be fully explored in the report due to time and resource limitations.

\subsection{Evaluation of Additional Engines}
\label{sec:additional}

Providing support for multiple kinds of datastore within the HUMS prototype proved to be a relatively straightforward process. As part of our prototype, we added both local and remote SQL, NoSQL and distributed Datastores, showing our architecture yields a high level of interoperability. The fact that we can switch between these by changing a configuration file at runtime, without any code being written, shows a high level of configurability and accessibility, since consumers with even low levels of technical knowledge can implement these changes.
 
The \texttt{FaceAnalysisEngine} provided a convincing example of a complex, domain-specific analysis engine being easily built on top of the generic interface. The configuration file allowed easy description of the faces to be accepted by storing a relative path to a face description file, and the Java interface provided a simple mechanism for retrieving data from the sensors. This engine showed that even highly tailored engines comfortably fit within our plugin model of engines, giving strong evidence that our prototype is highly tailorable.

% TODO Cover Availability, Security and Performance quality attributes if possible

%-------------------------------------------------------------%
%---------------------COMMUNICATION-------------------%
%-------------------------------------------------------------%
\section{Customer Communication}
\label{sec:customer_comms}

We maintained communication with the Customer throughout the phase of this report, with emails exchanged every few weeks. The first feedback received from the Customer was the feedback to the previous Interim Report. This was in the form of written feedback and a face-to-face meeting with the Customer, with the main points raised for improvement surrounding the risks and the quality attribute scenarios. This feedback was taken on board and incorporated into this report, with risks more thoroughly considered throughout the process and with the scenarios explicitly defined.

During email communication with the Customer, we attempted to resolve an issue they had with accessing the web sources that were produced as part of the Interim Report. Once this was done we briefly discussed the ideas behind the configurable engines architecture and it was agreed that it was a good strategy. We then asked about different qualities expected of the system, particularly configurability and its related qualities. The Customer replied highlighting configurability as a key quality and this helped to influence some of our development to ensure that this was well met.

Further information about how development was progressing and the proposals for future work was exchanged to keep the Customer up-to-date and ensure that it was on the right track. This included information about how the system would meet the previously highlighted configurability quality, and how this could be achieved at both compile time and runtime. Discussion about Datastores was also useful and revealed the possibility of additionally using a distributed database alongside the other Datastores we had implemented gateways for. This feedback was taken on board and \hl{xxx data store was yyy}.


%-------------------------------------------------------------%
%----------------------CONCLUSION----------------------%
%-------------------------------------------------------------%
\section{Conclusion}
\label{sec:conclusion}
\hl{TODO All}

\subsection{Summary}
\label{sec:summary}
\hl{TODO All}
This report has set out the details for the design and development of our HUMS, and in particular highlights the changes and progression from the previous report. The main addition from the previous report were requirements, the set of external dependencies and the design of the system architecture. The risks to the project, as well as mitigation and contingencies for these risks have been shown, as well as an indepth consideration of the quality attributes that make up the project.

The project management methodology has been shown and the different teams which worked together to build the core have been laid out and roles of each team explained.

The testing plan has been laid out and steps for acceptence testing and the methodology behind manual and automated testing of the prototypes.

The prototype systems, such as the test applications like the earthquake monitor, for the HUMS system have been demonstrated and the validation for the functional and non-functional requirements, as well as the quality of the solution, have been considered. The prototypes have been evaluated to check if they meet these requirements and qualities.


\subsection{Reflection}
\label{sec:reflection}
\hl{TODO All}

\bibliography{report-refs}
\bibliographystyle{IEEEtran}
\end{document}

